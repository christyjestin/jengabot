{"cells":[{"cell_type":"markdown","metadata":{"id":"DfPPQ6ztJhv4","cell_id":"2279f69fa46e4185a1fe709cd6bcee91","deepnote_cell_type":"markdown"},"source":"# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n","block_group":"64364358b1b54fe084c0a94ae8e1f1eb"},{"cell_type":"code","metadata":{"id":"DBIoe_tHTQgV","cell_id":"53f9939697ab4d5f891e515c80b818e0","deepnote_cell_type":"code"},"source":"!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n\n# Download TorchVision repo to use some files from\n# references/detection\n!git clone https://github.com/pytorch/vision.git\n!cd vision && git checkout v0.3.0\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./\n\nfrom manipulation import running_as_notebook\n\n# Imports\nimport fnmatch\nimport json\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom IPython.display import display\n\nimport torch\nimport torch.utils.data\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]\n\n# drake_reserved_labels = [32765, 32764, 32766, 32767]\n\n\ndef colorize_labels(image):\n    \"\"\"Colorizes labels.\"\"\"\n    cc = mpl.colors.ColorConverter()\n    color_cycle = plt.rcParams[\"axes.prop_cycle\"]\n    colors = np.array([cc.to_rgb(c[\"color\"]) for c in color_cycle])\n    bg_color = [0, 0, 0]\n    image = np.squeeze(image)\n    background = np.zeros(image.shape[:2], dtype=bool)\n    for label in reserved_labels:\n        background |= image == int(label)\n    foreground = image[np.logical_not(background)]\n    color_image = colors[image % len(colors)]\n    color_image[background] = bg_color\n    return color_image","block_group":"380c48fd6b994264bb67a36af4040cd8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwyE5A8DGtct","cell_id":"99672fc01f0d44e680952f463afb3d74","deepnote_cell_type":"markdown"},"source":"# Download our bin-picking dataset\n\nIt's definitely possible to actually create this dataset on Colab; I've just written a version of the \"clutter_gen\" method from the last chapter that writes the images (and label images) to disk, along with some annotations.  But it takes a non-trivial amount of time to generate 10,000 images. \n","block_group":"4f5870e140e045bb9af2fd670e6a4cfc"},{"cell_type":"code","metadata":{"id":"_DgAgqauIET9","cell_id":"18f9dd62b465432994e1024b2b59cd43","deepnote_cell_type":"code"},"source":"dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_data.zip .\n    !unzip -q clutter_maskrcnn_data.zip","block_group":"208962715f6c438db97d443156c1e821","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xA8sBvuHNNH1","cell_id":"2b22b402b9354083b1e2683b89ca944e","deepnote_cell_type":"markdown"},"source":"If you are on colab, go ahead and use the file browser on the left (looks like a drive under the table of contents panel) to click through the .png and .json files to make sure you understand the dataset you've just created!  If you're on a local machine, just browse to the folder.","block_group":"77bcf906e6124bf4b526e47087c19e24"},{"cell_type":"markdown","metadata":{"id":"C9Ee5NV54Dmj","cell_id":"aa1a8673164d4fd1b5a0219cee911ca5","deepnote_cell_type":"markdown"},"source":"# Teach pytorch how to load the dataset\n\ninto the [format expected by Mask R-CNN](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.maskrcnn_resnet50_fpn).","block_group":"c60240fe510e4bb094dd0c0be92d34e0"},{"cell_type":"code","metadata":{"id":"mTgWtixZTs3X","cell_id":"1b7b07b45d4348d3aa19f20ff42f3e16","deepnote_cell_type":"code"},"source":"class BinPickingDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.num_images = len(fnmatch.filter(os.listdir(root), \"*.png\"))\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        filename_base = os.path.join(self.root, f\"{idx:05d}\")\n\n        img = Image.open(filename_base + \".png\").convert(\"RGB\")\n        mask = np.squeeze(np.load(filename_base + \"_mask.npy\"))\n\n        with open(filename_base + \".json\", \"r\") as f:\n            instance_id_to_class_name = json.load(f)\n        labels = ycb == instance_id_to_class_name\n\n        # instances are encoded as different colors\n        obj_ids = np.asarray(list(instance_id_to_class_name.keys()))\n        count = (\n            (mask == np.int16(obj_ids)[:, None, None]).sum(axis=2).sum(axis=1)\n        )\n\n        # discard objects instances with less than 10 pixels\n        obj_ids = obj_ids[count >= 10]\n\n        labels = [\n            ycb.index(instance_id_to_class_name[id] + \".sdf\") for id in obj_ids\n        ]\n        obj_ids = np.int16(np.asarray(obj_ids))\n\n        # split the color-encoded mask into a set of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return self.num_images","block_group":"65a8f0b85da9489dbc2e33c340037f27","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6f3ZOTJ4Km9","cell_id":"7577df2d1fb7452db15f53261f8e60ab","deepnote_cell_type":"markdown"},"source":"Let's check the output of our dataset.","block_group":"4118604e58274292abffb2c9c20d58d0"},{"cell_type":"code","metadata":{"id":"ZEARO4B_ye0s","cell_id":"f92e52fac701472e9fbaa0f468aaaf34","deepnote_cell_type":"code"},"source":"dataset = BinPickingDataset(dataset_path)\ndataset[0][0]","block_group":"94b772d55c18417faa06f2802948f5cc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xWA2NXwVhV_C","cell_id":"6986d374dc6a4dab83ccfc62b1119b37","deepnote_cell_type":"markdown"},"source":"# Define the network\n\nThis cell is where the magic begins to happen.  We load a network that is pre-trained on the COCO dataset, then replace the network head with a new (untrained) network with the right number of outputs for our YCB recognition/segmentation task.","block_group":"b991a296f17a4da5a2debc0e49ea3d7c"},{"cell_type":"code","metadata":{"id":"YjNHjVMOyYlH","cell_id":"37a4c826fbba43a98152dab72e87addc","deepnote_cell_type":"code"},"source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model","block_group":"0decc0e0736749f0a1d7450660484b01","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WXLwePV5ieP","cell_id":"cc6d46ae1f374cb2a4437ed0207c939b","deepnote_cell_type":"markdown"},"source":"That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n\n# Transforms\n\nLet's write some helper functions for data augmentation / transformation, which leverages the functions in torchvision `refereces/detection`. \n","block_group":"0bf679744c6a4437af61a391b116e52b"},{"cell_type":"code","metadata":{"id":"l79ivkwKy357","cell_id":"ffdab69377104d3395140c0e0783abc5","deepnote_cell_type":"code"},"source":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","block_group":"22d28ee13b3440a399e91e39d3ccaafb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzCLqiZk-sjf","cell_id":"40d279e6bf5f423b82824adfdcef162a","deepnote_cell_type":"markdown"},"source":"Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.","block_group":"da1b4411995844699836778776a877b5"},{"cell_type":"markdown","metadata":{"id":"3YFJGJxk6XEs","cell_id":"2634062e31a34134b8d3ed7386cc7ef7","deepnote_cell_type":"markdown"},"source":"# Putting everything together\n\nWe now have the dataset class, the models and the data transforms. Let's instantiate them","block_group":"56b9971ac0c848fcb61ad805c265ab31"},{"cell_type":"code","metadata":{"id":"a5dGaIezze3y","cell_id":"43eb26a271414440bec02c1f22bf003f","deepnote_cell_type":"code"},"source":"# use our dataset and defined transformations\ndataset = BinPickingDataset(dataset_path, get_transform(train=True))\ndataset_test = BinPickingDataset(dataset_path, get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=utils.collate_fn,\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=utils.collate_fn,\n)","block_group":"278d2097ad5046149ecee2183abe6815","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5yvZUprj4ZN","cell_id":"08ca22971d1c4da3a19d0ab617d52c97","deepnote_cell_type":"markdown"},"source":"Now let's instantiate the model and the optimizer","block_group":"d9d9189e3a4246368dd3cbccc5f91e40"},{"cell_type":"code","metadata":{"id":"zoenkCj18C4h","cell_id":"061f227331cc419aa6504e989190d98f","deepnote_cell_type":"code"},"source":"device = (\n    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n)\n\nnum_classes = len(ycb) + 1\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params, lr=0.005, momentum=0.9, weight_decay=0.0005\n)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer, step_size=3, gamma=0.1\n)","block_group":"a829d2149cc1468ebc012b4ab28ebad7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAd56lt4kDxc","cell_id":"ed84c90a270149599cb72b9f4b573e00","deepnote_cell_type":"markdown"},"source":"And now let's train the model for 10 epochs, evaluating at the end of every epoch.","block_group":"6116c497b09e40c292fb17efd4953e01"},{"cell_type":"code","metadata":{"id":"at-h4OWK0aoc","cell_id":"e41cbcb80ab44e43883e09e208029c48","deepnote_cell_type":"code"},"source":"# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(\n        model, optimizer, data_loader, device, epoch, print_freq=10\n    )\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","block_group":"d0d8e35b53d141a2a66ae7488b89b525","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXTyZhCScUTI","cell_id":"736fbd611eaf4e64b406272129d54a2d","deepnote_cell_type":"markdown"},"source":"If you're going to leave this running for a bit, I recommend scheduling the following cell to run immediately (so that you don't lose your work).","block_group":"7d76421512d14fe49dc0644629c0ef7a"},{"cell_type":"code","metadata":{"id":"vUJXn15pGzRj","cell_id":"1af026d2d25e443993a77648c20080cd","deepnote_cell_type":"code"},"source":"torch.save(model.state_dict(), \"clutter_maskrcnn_model.pt\")\n\nfrom google.colab import files\n\nfiles.download(\"clutter_maskrcnn_model.pt\")","block_group":"3650c3ae992045099927288901bedc96","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6mYGFLxkO8F","cell_id":"ea70a3e357804e8f8cd02764825cbf74","deepnote_cell_type":"markdown"},"source":"Now that training has finished, let's have a look at what it actually predicts in a test image","block_group":"cf7e18bfd24548c2bcfb14fcdb4117e8"},{"cell_type":"code","metadata":{"id":"YHwIdxH76uPj","cell_id":"d56c0717679b4f79a1324a67be0590d7","deepnote_cell_type":"code"},"source":"# pick one image from the test set\nimg, _ = dataset_test[0]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])","block_group":"b0673218ca1b437ab93ee647b71bbdfe","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmN602iKsuey","cell_id":"1b6ae8482ffa4f5ab8c4659485c60995","deepnote_cell_type":"markdown"},"source":"Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\nThe dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields.","block_group":"3b247ec05ec2407c91980e29ab9b3fcc"},{"cell_type":"code","metadata":{"id":"Lkmb3qUu6zw3","cell_id":"4a18eb369bd149ed96054eb1fa9efc80","deepnote_cell_type":"code"},"source":"prediction","block_group":"19ac83450686474cb1601a8040af9f8b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwT21rzotFbH","cell_id":"347781b029924c8d910e2fb80ca3f035","deepnote_cell_type":"markdown"},"source":"Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","block_group":"4ce563b690e347c9bd48c4f3ccc1f7e9"},{"cell_type":"code","metadata":{"id":"bpqN9t1u7B2J","cell_id":"f78a4ff318fc4c38ab19ad0facab7aec","deepnote_cell_type":"code"},"source":"Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())","block_group":"5227656246ba4afa822fa6a454bc216f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M58J3O9OtT1G","cell_id":"faf21d84866c495888bef0a7b891f80a","deepnote_cell_type":"markdown"},"source":"And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.","block_group":"32c4e3c7e3ac4173bcbad4b543d39207"},{"cell_type":"code","metadata":{"id":"5v5S3bm07SO1","cell_id":"19f728ee0add4b43be7948cc1f0c4153","deepnote_cell_type":"code"},"source":"Image.fromarray(prediction[0][\"masks\"][0, 0].mul(255).byte().cpu().numpy())","block_group":"f1334f1a74b3431e91868377de6d1d05","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c91043b1-e029-4b16-9924-9f7aca75a010' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clutter_maskrcnn_train.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.10.8 64-bit"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.8","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"4423920e16f74619a0a7d0886a5b6d42","deepnote_execution_queue":[]}}