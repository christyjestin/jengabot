{"cells":[{"cell_type":"markdown","metadata":{"id":"VEYe67K6E6j0","cell_id":"76462156afda4830b00dcd7d71eaf9dc","deepnote_cell_type":"markdown"},"source":"# Antipodal Grasp with Deep Segmentation\nIn this problem, you will use same the antipodal grasp strategy we used with geometric perception, but you'll use deep perception to restrict your grasps to a single object (the mustard bottle).\n\nWe'll be using the Mask RCNN model that's trained in [this script](https://colab.research.google.com/github/RussTedrake/manipulation/blob/master/segmentation_train.ipynb) from the textbook. As an input, the model takes an image, and it outputs a series of masks showing where the objects we've trained it on are in the image. (In this case, those objects are images in the YCB dataset.) Once we know which pixels contain the object we wish to grasp, then we can project them back out to point clouds using the depth image and select an antipodal grasp using just those data points.\n\nYour job in this notebook will be to use the masks output by our neural network to filter the point cloud to only include points on the mustart bottle. Once we have a filtered point cloud, we'll be able to use them to generate antipodal grasps just on our object of interest.","block_group":"00000-920cf414-0bf7-4133-b265-04f55b6c5ad9"},{"cell_type":"code","metadata":{"id":"v5OrhpSmxkGH","source_hash":"ac7c30ed","execution_start":1634775608309,"execution_millis":910,"deepnote_to_be_reexecuted":false,"cell_id":"b557557f51b1404ba4fea2637471e4cc","deepnote_cell_type":"code"},"source":"import os\nfrom copy import deepcopy\nfrom urllib.request import urlretrieve\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport torchvision\nimport torchvision.transforms.functional as Tf\nfrom IPython.display import clear_output, display\nfrom pydrake.all import (\n    BaseField,\n    Concatenate,\n    Fields,\n    MeshcatVisualizer,\n    MeshcatVisualizerParams,\n    PointCloud,\n    Quaternion,\n    Rgba,\n    RigidTransform,\n    RotationMatrix,\n    StartMeshcat,\n)\nfrom pydrake.multibody.parsing import Parser\nfrom pydrake.multibody.plant import AddMultibodyPlantSceneGraph\nfrom pydrake.systems.framework import DiagramBuilder\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nfrom manipulation import running_as_notebook\nfrom manipulation.clutter import GenerateAntipodalGraspCandidate\nfrom manipulation.scenarios import AddRgbdSensors\nfrom manipulation.utils import (\n    AddPackagePaths,\n    ConfigureParser,\n    LoadDataResource,\n)","block_group":"00001-0c67af8c-9b86-4865-8896-124b095f8bd7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"6d42057f","execution_start":1634775265327,"execution_millis":5,"deepnote_to_be_reexecuted":false,"cell_id":"e30b5131dfaf4e48ae065cd1789d56a5","deepnote_cell_type":"code"},"source":"# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"00002-01dba1b8-8757-420a-9d1f-a2ced800e090","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"24e3bc40e1a7498190f1c44c90df094f","deepnote_cell_type":"markdown"},"source":"## Load Model\nTo avoid making you wait and train the model yourself, we'll use the pre-trained model from the textbook. First, we need to load it.","block_group":"00002-b652ea38-6691-4cbc-a79e-f354c265451a"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"dfb97091","execution_start":1634775259173,"execution_millis":1,"deepnote_to_be_reexecuted":false,"cell_id":"ab8f6a39e86f466d9d35ae76c67c6b28","deepnote_cell_type":"code"},"source":"if running_as_notebook:\n    model_file = \"clutter_maskrcnn_model.pt\"\n    if not os.path.exists(model_file):\n        urlretrieve(\n            \"https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt\",\n            model_file,\n        )","block_group":"00002-618b2559-af15-4389-8a57-4a48e19ebabb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"8da585f9","execution_start":1634775259179,"execution_millis":5153,"deepnote_to_be_reexecuted":false,"cell_id":"199e97649d3647099676b63ee86ad244","deepnote_cell_type":"code"},"source":"mustard_ycb_idx = 3\nif running_as_notebook:\n\n    def get_instance_segmentation_model(num_classes):\n        # load an instance segmentation model pre-trained on COCO\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n            weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT, progress=False\n        )\n\n        # get the number of input features for the classifier\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        # replace the pre-trained head with a new one\n        model.roi_heads.box_predictor = FastRCNNPredictor(\n            in_features, num_classes\n        )\n\n        # now get the number of input features for the mask classifier\n        in_features_mask = (\n            model.roi_heads.mask_predictor.conv5_mask.in_channels\n        )\n        hidden_layer = 256\n        # and replace the mask predictor with a new one\n        model.roi_heads.mask_predictor = MaskRCNNPredictor(\n            in_features_mask, hidden_layer, num_classes\n        )\n\n        return model\n\n    num_classes = 7\n    model = get_instance_segmentation_model(num_classes)\n    device = (\n        torch.device(\"cuda\")\n        if torch.cuda.is_available()\n        else torch.device(\"cpu\")\n    )\n    model.load_state_dict(\n        torch.load(\"clutter_maskrcnn_model.pt\", map_location=device)\n    )\n    model.eval()\n\n    model.to(device)","block_group":"00004-31a83327-6649-450a-8f0f-9c0d07268a4f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7b89fd30f0a9416885bf6a5e184cb00f","deepnote_cell_type":"markdown"},"source":"## Set Up Camera System\nNow that we've loaded our network, we need to set up the Drake model for our system. It has several objects from the YCB data set and two cameras.","block_group":"00005-62942de5-8106-48b0-a145-b4bc58840231"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"bd2f0581","execution_start":1634775269991,"execution_millis":3,"deepnote_to_be_reexecuted":false,"cell_id":"ef7178bede814c68a82ab037689702e8","deepnote_cell_type":"code"},"source":"def ClutteredSceneSystem():\n    builder = DiagramBuilder()\n\n    # Create the physics engine + scene graph.\n    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.0)\n    parser = Parser(plant)\n    ConfigureParser(parser)\n    parser.AddModelsFromUrl(\n        \"package://manipulation/segmentation_and_grasp_scene.dmd.yaml\"\n    )\n    plant.Finalize()\n\n    AddRgbdSensors(builder, plant, scene_graph)\n\n    diagram = builder.Build()\n    diagram.set_name(\"depth_camera_demo_system\")\n    context = diagram.CreateDefaultContext()\n    return diagram, context","block_group":"00007-364e871f-95a0-4ea6-8668-b176a902edbd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"b566af18","execution_start":1634775312935,"execution_millis":2974,"deepnote_to_be_reexecuted":false,"cell_id":"9d7c716e3b0d468684d990f87df7751a","deepnote_cell_type":"code"},"source":"class CameraSystem:\n    def __init__(self, idx, meshcat, diagram, context):\n        self.idx = idx\n\n        # Read images\n        depth_im_read = (\n            diagram.GetOutputPort(\"camera{}_depth_image\".format(idx))\n            .Eval(context)\n            .data.squeeze()\n        )\n        self.depth_im = deepcopy(depth_im_read)\n        self.depth_im[self.depth_im == np.inf] = 10.0\n        self.rgb_im = (\n            diagram.GetOutputPort(\"camera{}_rgb_image\".format(idx))\n            .Eval(context)\n            .data\n        )\n\n        # Visualize\n        point_cloud = diagram.GetOutputPort(\n            \"camera{}_point_cloud\".format(idx)\n        ).Eval(context)\n        meshcat.SetObject(f\"Camera {idx} point cloud\", point_cloud)\n\n        # Get other info about the camera\n        cam = diagram.GetSubsystemByName(\"camera\" + str(idx))\n        cam_context = cam.GetMyMutableContextFromRoot(context)\n        self.X_WC = cam.body_pose_in_world_output_port().Eval(cam_context)\n        self.cam_info = cam.depth_camera_info()\n\n    def project_depth_to_pC(self, depth_pixel):\n        \"\"\"\n        project depth pixels to points in camera frame\n        using pinhole camera model\n        Input:\n            depth_pixels: numpy array of (nx3) or (3,)\n        Output:\n            pC: 3D point in camera frame, numpy array of (nx3)\n        \"\"\"\n        # switch u,v due to python convention\n        v = depth_pixel[:, 0]\n        u = depth_pixel[:, 1]\n        Z = depth_pixel[:, 2]\n        cx = self.cam_info.center_x()\n        cy = self.cam_info.center_y()\n        fx = self.cam_info.focal_x()\n        fy = self.cam_info.focal_y()\n        X = (u - cx) * Z / fx\n        Y = (v - cy) * Z / fy\n        pC = np.c_[X, Y, Z]\n        return pC\n\n\nenvironment_diagram, environment_context = ClutteredSceneSystem()\ncameras = []\ncameras.append(\n    CameraSystem(0, meshcat, environment_diagram, environment_context)\n)\ncameras.append(\n    CameraSystem(1, meshcat, environment_diagram, environment_context)\n)","block_group":"00006-744be7ee-0dfb-4121-a1f1-da60dc7452d7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"14713ab1681d45579d1d2a5ae963bca6","deepnote_cell_type":"markdown"},"source":"### Examining Camera Views\nIf you take a look at Meshcat, under \"scene > drake,\" you'll find two check boxes: one labeled \"Camera 0 point cloud\" and one labeled \"Camera 1 point cloud.\" Toggle these to see the different views the two cameras get.\n\nWe can also look directly at the images captured by each camera:","block_group":"00009-aee25f24-10f0-497d-a986-fabdc70d21ef"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"435da067","execution_start":1634775333501,"execution_millis":735,"deepnote_to_be_reexecuted":false,"cell_id":"1cd704841777450b9e8a130720bc8287","deepnote_cell_type":"code"},"source":"plt.imshow(cameras[0].rgb_im)\nplt.title(\"View from camera 0\")\nplt.show()\n\nplt.imshow(cameras[1].rgb_im)\nplt.title(\"View from camera 1\")\nplt.show()","block_group":"00009-ec9221b8-cd7c-40df-b7e9-341c48471772","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e292494ea07f41af8bfc7c8ef49e68fa","deepnote_cell_type":"markdown"},"source":"## Generate masks for each image\nNow that we have a network and camera inputs, we can start processing our inputs. First, we will evaluate the mask (which is the output from our network) for each image.","block_group":"00010-b805f9aa-396d-40ff-ba92-8be4fb8a3ebe"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"66ca9127","execution_start":1634775338291,"execution_millis":33005,"deepnote_to_be_reexecuted":false,"cell_id":"41ee3b128a584c839fa34daa4aec53f6","deepnote_cell_type":"code"},"source":"if running_as_notebook:\n    with torch.no_grad():\n        predictions = []\n        predictions.append(\n            model([Tf.to_tensor(cameras[0].rgb_im[:, :, :3]).to(device)])\n        )\n        predictions.append(\n            model([Tf.to_tensor(cameras[1].rgb_im[:, :, :3]).to(device)])\n        )\n    for i in range(2):\n        for k in predictions[i][0].keys():\n            if k == \"masks\":\n                predictions[i][0][k] = (\n                    predictions[i][0][k].mul(255).byte().cpu().numpy()\n                )\n            else:\n                predictions[i][0][k] = predictions[i][0][k].cpu().numpy()\nelse:\n    predictions = []\n    for i in range(2):\n        prediction = []\n        prediction.append(\n            np.load(LoadDataResource(\"prediction_{}.npz\".format(i)))\n        )\n        predictions.append(prediction)","block_group":"00011-0ae55ef2-9d2f-42e1-b9bb-abbd949955c4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ca9470e50e814cc795a735593624b013","deepnote_cell_type":"markdown"},"source":"`predictions[0]` was run on the image from Camera 0, while `predictions[1]` was run on the image from Camera 1. \n\nLets take a minute to understand this output. Breaking it down by each key in the output dictionary:\n* The \"boxes\" correspond to bounding boxes on the regions containing the object\n* The \"labels\" tell us which class the model has associated with the (as in, whether it's the mustard bottle, the Cheez-it box, the spam container, etc. Each model is identified by a number.)\n* The \"scores\" are a measure of confidence in the model predictions\n* The \"masks\" are arrays which indicate which pixels belong to the corresponding class","block_group":"00013-2a5173f9-5b60-4e65-8d32-448d6f2b5b27"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"2f700070","execution_start":1634775371302,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"46d17fee40304ca49407ffd4b929a134","deepnote_cell_type":"code"},"source":"predictions[0]","block_group":"00012-e4772258-294a-4dc0-a9d5-24fdbbf64535","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c4a0a252824b492ba7702eb21c2ae8e6","deepnote_cell_type":"markdown"},"source":"The two most important elements for our task are `labels`, which tells us which class the mask corresponds to, and `mask`, which gives a higher score for pixels that more likely correspond to points on the mustard bottle.\n\nNote that we defined `mustard_ycb_idx = 3` at the top of the notebook; that's the value of the label for the class we care about.\n\nThe following cells visualize the masks we get:","block_group":"00016-a6e730fc-27a0-4a8c-b006-5900d30916dc"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"d6e382b2","execution_start":1634764705722,"execution_millis":714,"deepnote_to_be_reexecuted":false,"cell_id":"e70974ed629548348bfed8e2881559d9","deepnote_cell_type":"code"},"source":"for i, prediction in enumerate(predictions):\n    mask_idx = np.argmax(predictions[i][0][\"labels\"] == mustard_ycb_idx)\n    mask = predictions[i][0][\"masks\"][mask_idx, 0]\n\n    plt.imshow(mask)\n    plt.title(\"Mask from Camera \" + str(i))\n    plt.colorbar()\n    plt.show()","block_group":"00015-dcc1e7ac-81d1-4ee3-ad03-70e9f91a68d2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"be3b8fbde8b6449cb912eb1c69b0dea5","deepnote_cell_type":"markdown"},"source":"## Generate Point Cloud\n### 9.2a Masking\nUsing the masks we've found, generate a filtered point cloud that includes images from both cameras but only includes points within the mustard. (You will fill in code to return both points and colors; technically, we only need the points for finding an antipodal grasp, but the colors are helpful for visualization.)\n\nYou will write code that does the following for each camera and image:\n1. Extract the pixels from the mask that we consider to be within the mustard bottle (specifically: take values that are above `mask_threshold`)\n2. Select points in the depth image corresponding to those pixels\n3. Using the depth values, project those selected pixels back out to be points in the camera frame. You'll use the camera's `project_depth_to_pC` image to do this; refer back to problem 5.2 for how this is used.\n4. Convert the points to the world frame\n5. Select color values from the RGB image that correspond to your mask pixels. ","block_group":"00014-4f1f90bd-b2e9-415a-a3bb-bbb0920105ae"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"a659f42e","execution_start":1634775642499,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"7ac1b78bcfcd41638fc4503695c11942","deepnote_cell_type":"code"},"source":"def get_merged_masked_pcd(\n    predictions,\n    rgb_ims,\n    depth_ims,\n    project_depth_to_pC_funcs,\n    X_WCs,\n    mask_threshold=150,\n):\n    \"\"\"\n    predictions: The output of the trained network (one for each camera)\n    rgb_ims: RGBA images from each camera\n    depth_ims: Depth images from each camera\n    project_depth_to_pC_funcs: Functions that perform the pinhole camera operations to convert pixels\n        into points. See the analogous function in problem 5.2 to see how to use it.\n    X_WCs: Poses of the cameras in the world frame\n    \"\"\"\n\n    pcd = []\n    for prediction, rgb_im, depth_im, project_depth_to_pC_func, X_WC in zip(\n        predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n    ):\n        # These arrays aren't the same size as the correct outputs, but we're\n        # just initializing them to something valid for now.\n        spatial_points = np.zeros((3, 1))\n        rgb_points = np.zeros((3, 1))\n\n        ######################################\n        # Your code here (populate spatial_points and rgb_points)\n        ######################################\n\n        # You get an unhelpful RunTime error if your arrays are the wrong\n        # shape, so we'll check beforehand that they're the correct shapes.\n        assert (\n            len(spatial_points.shape) == 2\n        ), \"Spatial points is the wrong size -- should be 3 x N\"\n        assert (\n            spatial_points.shape[0] == 3\n        ), \"Spatial points is the wrong size -- should be 3 x N\"\n        assert (\n            len(rgb_points.shape) == 2\n        ), \"RGB points is the wrong size -- should be 3 x N\"\n        assert (\n            rgb_points.shape[0] == 3\n        ), \"RGB points is the wrong size -- should be 3 x N\"\n        assert rgb_points.shape[1] == spatial_points.shape[1]\n\n        N = spatial_points.shape[1]\n        pcd.append(PointCloud(N, Fields(BaseField.kXYZs | BaseField.kRGBs)))\n        pcd[-1].mutable_xyzs()[:] = spatial_points\n        pcd[-1].mutable_rgbs()[:] = rgb_points\n        # Estimate normals\n        pcd[-1].EstimateNormals(radius=0.1, num_closest=30)\n        # Flip normals toward camera\n        pcd[-1].FlipNormalsTowardPoint(X_WC.translation())\n\n    # Merge point clouds.\n    merged_pcd = Concatenate(pcd)\n\n    # Voxelize down-sample.  (Note that the normals still look reasonable)\n    return merged_pcd.VoxelizedDownSample(voxel_size=0.005)","block_group":"00018-757ddfc4-b93d-4e38-be05-63fea943c8ce","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"05e10a0aeb1941688fb00e64d055abd7","deepnote_cell_type":"markdown"},"source":"Now let's use this function to visualize the output of `get_merged_masked_pcd`.","block_group":"00020-ed664400-7d74-479c-8283-826c1dc1fe04"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"a7245486","execution_start":1634775649507,"execution_millis":636,"deepnote_to_be_reexecuted":false,"cell_id":"63032e7dce2743268e9898140c478d60","deepnote_cell_type":"code"},"source":"rgb_ims = [c.rgb_im for c in cameras]\ndepth_ims = [c.depth_im for c in cameras]\nproject_depth_to_pC_funcs = [c.project_depth_to_pC for c in cameras]\nX_WCs = [c.X_WC for c in cameras]\n\npcd = get_merged_masked_pcd(\n    predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n)\nmeshcat.SetObject(\"masked_cloud\", pcd, point_size=0.003)","block_group":"00019-2970c0b1-0d69-47a0-8ea0-b58d33be190d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"26d6853cb4f543d79110bf5869a30f0c","deepnote_cell_type":"markdown"},"source":"## Select a grasp\nThe following code uses your point cloud function to find an antipodal grasp, similar to an exercise on a previous problem set.","block_group":"00022-77955a70-4e3b-433e-b889-3fdaeaa6de9f"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"e12f6757","execution_start":1634775673810,"execution_millis":2856,"deepnote_to_be_reexecuted":false,"cell_id":"862c4491bdf04ddb80965a412c92a5f4","deepnote_cell_type":"code"},"source":"def find_antipodal_grasp(environment_diagram, environment_context, cameras):\n    rng = np.random.default_rng()\n\n    # Another diagram for the objects the robot \"knows about\": gripper, cameras, bins.  Think of this as the model in the robot's head.\n    builder = DiagramBuilder()\n    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.001)\n    parser = Parser(plant)\n    ConfigureParser(parser)\n    parser.AddModelsFromUrl(\n        \"package://manipulation/schunk_wsg_50_welded_fingers.dmd.yaml\"\n    )\n    plant.Finalize()\n\n    params = MeshcatVisualizerParams()\n    params.prefix = \"planning\"\n    visualizer = MeshcatVisualizer.AddToBuilder(\n        builder, scene_graph, meshcat, params\n    )\n    diagram = builder.Build()\n    context = diagram.CreateDefaultContext()\n    diagram.ForcedPublish(context)\n\n    rgb_ims = [c.rgb_im for c in cameras]\n    depth_ims = [c.depth_im for c in cameras]\n    project_depth_to_pC_funcs = [c.project_depth_to_pC for c in cameras]\n    X_WCs = [c.X_WC for c in cameras]\n\n    cloud = get_merged_masked_pcd(\n        predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n    )\n\n    plant_context = plant.GetMyContextFromRoot(context)\n    scene_graph_context = scene_graph.GetMyContextFromRoot(context)\n\n    min_cost = np.inf\n    best_X_G = None\n    for i in range(100):\n        cost, X_G = GenerateAntipodalGraspCandidate(\n            diagram, context, cloud, rng\n        )\n        if np.isfinite(cost) and cost < min_cost:\n            min_cost = cost\n            best_X_G = X_G\n\n    plant.SetFreeBodyPose(plant_context, plant.GetBodyByName(\"body\"), best_X_G)\n    diagram.ForcedPublish(context)\n\n\nif running_as_notebook:\n    find_antipodal_grasp(environment_diagram, environment_context, cameras)","block_group":"00012-68407111-65bb-4d35-8774-f031944d846b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"a6cac13383ee4f9884472140e20188df","deepnote_cell_type":"markdown"},"source":"## Summary\nIf you take a look at Meshcat, we've reached our end goal of generating an antipodal grasp! We've now shown that we can leverage the RGB data and our trained network to filter our point clouds and get an antipodal grasp for a specific object. Now, let's think about the implications of some of the design choices we made along the way.","block_group":"00024-34ea2152-8485-47d3-9220-a631c2b4b5dd"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"15ebbbabf1994dec809fa2ee8a68d82f","deepnote_cell_type":"markdown"},"source":"## Written Questions\nAnswer the following questions in your written submission for this problem set.\n\n#### 9.2b Number of Cameras\nLet's think back to the \"Examining camera views\" section. Toggling between the views of the two cameras, each of the cameras contributes different information about the scene. Why do we need the information from both of them to find antipodal grasps?","block_group":"00025-7372c0a0-1338-4698-88d0-a441dc9fbc63"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c8b4c99d4c1c40a9bf8100db7521584e","deepnote_cell_type":"markdown"},"source":"#### 9.2c Why Segment for Grasp Selection?\nOur goal in this task is to grasp the mustard bottle. The first step of `get_merged_masked_pcd()` was to extract the pixels that correspond to the mustard bottle. If we skipped this step and instead considered the entire point cloud, what type of \"undesirable\" grasps might we select?","block_group":"00026-9d77d75b-1372-433c-a316-6f325798d3a2"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"6715c0da89c646c59caa8db766985f71","deepnote_cell_type":"markdown"},"source":"#### 9.2d Unsegmented Point Cloud\nIn this notebook, after we mask the point clouds based on the segmentation results, we don't use the other point clouds again (including when we evaluate our grasp candidates). Think about how we have checked whether or not a grasp is feasible. How might discarding the point cloud data for the other objects inadvertently lead us to select an invalid grasp?","block_group":"00028-3aab55cc-f1ff-4377-b1fc-38fd53324d15"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"a19401a37c7c440aaaaf8da6269ca237","deepnote_cell_type":"markdown"},"source":"## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza. \n\nFor submission of this assignment, you must do two things. \n- Download and submit the notebook `segmentation_and_grasp.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n- Answer parts (b) through (d) in the written section of Gradescope as a part of your `pdf` submission. \n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [4 pts] `get_merged_masked_pcd` must be implemented correctly. \n- [2 pts] Correct answer for 9.2b\n- [2 pts] Correct answer for 9.2c\n- [2 pts] Correct answer for 9.2d","block_group":"00030-217ead8e-a18a-479d-9e1b-750ff9d7f3ee"},{"cell_type":"code","metadata":{"tags":[],"source_hash":"68fcc6d0","execution_start":1634764711838,"execution_millis":993,"deepnote_to_be_reexecuted":false,"cell_id":"6eda88eae8e94a2d9a92a8131ab19194","deepnote_cell_type":"code"},"source":"from manipulation.exercises.segmentation.test_segmentation_and_grasp import (\n    TestSegmentationAndGrasp,\n)\nfrom manipulation.exercises.grader import Grader\n\nGrader.grade_output([TestSegmentationAndGrasp], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"00031-3b982245-2b72-4581-9494-7f0a290ec2ef","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c91043b1-e029-4b16-9924-9f7aca75a010' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"grasp_candidate.ipynb","provenance":[],"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.10.8 64-bit"},"celltoolbar":"Tags","language_info":{"name":"python","version":"3.10.8","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"17d483fd93c747a5abe6257ce08bb8a9","deepnote_execution_queue":[]}}