{"cells":[{"cell_type":"markdown","metadata":{"id":"DfPPQ6ztJhv4","cell_id":"2a7dea62a32646799012b6fcb5beb92b","deepnote_cell_type":"markdown"},"source":"# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n","block_group":"29fced2b64e64799a3f3421b80c3b64f"},{"cell_type":"code","metadata":{"id":"DBIoe_tHTQgV","cell_id":"04c4b34076eb480abf86b581aafd1ae5","deepnote_cell_type":"code"},"source":"# Imports\nimport fnmatch\nimport json\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom IPython.display import display\n\nimport torch\nimport torch.utils.data\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]","block_group":"aae2d454bc41490aa8ea2b7f5337de72","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwyE5A8DGtct","cell_id":"d3fb769d87d54c4aa6493c5fe547a97f","deepnote_cell_type":"markdown"},"source":"# Download our bin-picking model\n\nAnd a small set of images for testing.","block_group":"fb09fd0e0e4b4091b650b5b055f742f7"},{"cell_type":"code","metadata":{"id":"_DgAgqauIET9","cell_id":"d2d580fa5ac34e2c8b8e08bd78e05b6b","deepnote_cell_type":"code"},"source":"dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip .\n    !unzip -q clutter_maskrcnn_test.zip\n\nnum_images = len(fnmatch.filter(os.listdir(dataset_path), \"*.png\"))\n\n\ndef open_image(idx):\n    filename = os.path.join(dataset_path, f\"{idx:05d}.png\")\n    return Image.open(filename).convert(\"RGB\")\n\n\nmodel_file = \"clutter_maskrcnn_model.pt\"\nif not os.path.exists(model_file):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt .","block_group":"091a5a099d7b472a9f7194ad684a34ca","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xA8sBvuHNNH1","cell_id":"0dad9f46a20942c9b088562602cbb412","deepnote_cell_type":"markdown"},"source":"# Load the model","block_group":"4728ab316f0142ea892406e79b0adc62"},{"cell_type":"code","metadata":{"id":"vUJXn15pGzRj","cell_id":"db402f6dd4c24c628b750cb1ccdf8e51","deepnote_cell_type":"code"},"source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nimport torchvision.transforms.functional as Tf\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model\n\n\nnum_classes = len(ycb) + 1\nmodel = get_instance_segmentation_model(num_classes)\ndevice = (\n    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n)\nmodel.load_state_dict(\n    torch.load(\"clutter_maskrcnn_model.pt\", map_location=device)\n)\nmodel.eval()\n\nmodel.to(device)","block_group":"ab46401452034b9e98726bd1b441dc5a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6mYGFLxkO8F","cell_id":"21e7861e90284f879d793597c03dd897","deepnote_cell_type":"markdown"},"source":"# Evaluate the network","block_group":"e0359f705b4240e598638ab674c68db5"},{"cell_type":"code","metadata":{"id":"YHwIdxH76uPj","cell_id":"692eee469e5c43a1810da52317b4e135","deepnote_cell_type":"code"},"source":"# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9952)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])","block_group":"4e4e22c818cc4dd4bef689982be70f3d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmN602iKsuey","cell_id":"27ad2150b32f4ddcad8dc2ef2df9627a","deepnote_cell_type":"markdown"},"source":"Printing the prediction shows that we have a list of dictionaries. Each element\nof the list corresponds to a different image; since we have a single image,\nthere is a single dictionary in the list. The dictionary contains the\npredictions for the image we passed. In this case, we can see that it contains\n`boxes`, `labels`, `masks` and `scores` as fields.","block_group":"49bdb93d9daf43219c46f9c0a8e7b0b5"},{"cell_type":"code","metadata":{"id":"Lkmb3qUu6zw3","cell_id":"6d8cab2635264e58829daf20d338a367","deepnote_cell_type":"code"},"source":"prediction","block_group":"1c96981236f7424690afd954d1ca212c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwT21rzotFbH","cell_id":"cdc2a5b76fdc4cc18b0a26705ab3d5eb","deepnote_cell_type":"markdown"},"source":"Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","block_group":"5c6bb3c0e89d4207882bfae5458b1dd1"},{"cell_type":"code","metadata":{"id":"bpqN9t1u7B2J","cell_id":"c657f133ea47413b9802efb2aa36ba52","deepnote_cell_type":"code"},"source":"img","block_group":"c225d31dbeb941f3b7e23c4a5255d48e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M58J3O9OtT1G","cell_id":"91c41e9658a94500bda1d16aaa65b91d","deepnote_cell_type":"markdown"},"source":"And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.","block_group":"48ed3f94d11646d0b9905341686dc08c"},{"cell_type":"code","metadata":{"id":"5v5S3bm07SO1","cell_id":"5910c231e9c54c0998b4b60936ca6e73","deepnote_cell_type":"code"},"source":"N = prediction[0][\"masks\"].shape[0]\nfig, ax = plt.subplots(N, 1, figsize=(15, 15))\nfor n in range(prediction[0][\"masks\"].shape[0]):\n    ax[n].imshow(\n        np.asarray(\n            Image.fromarray(\n                prediction[0][\"masks\"][n, 0].mul(255).byte().cpu().numpy()\n            )\n        )\n    )","block_group":"a80b4474df9c4078bede3364180582b9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9QAeX9HkDTx","cell_id":"7d3fe9c0b25d43bca48feef71708683d","deepnote_cell_type":"markdown"},"source":"# Plot the object detections","block_group":"c4237a3bcaa34465b7f5a3e38c3e50f7"},{"cell_type":"code","metadata":{"id":"Z08keVFkvtPh","cell_id":"edb1b062e12a40139cf7e641a45eeb54","deepnote_cell_type":"code"},"source":"import matplotlib.patches as patches\nimport random\n\n\ndef plot_prediction():\n    img_np = np.array(img)\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(img_np)\n\n    cmap = plt.get_cmap(\"tab20b\")\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    num_instances = prediction[0][\"boxes\"].shape[0]\n    bbox_colors = random.sample(colors, num_instances)\n    boxes = prediction[0][\"boxes\"].cpu().numpy()\n    labels = prediction[0][\"labels\"].cpu().numpy()\n\n    for i in range(num_instances):\n        color = bbox_colors[i]\n        bb = boxes[i, :]\n        bbox = patches.Rectangle(\n            (bb[0], bb[1]),\n            bb[2] - bb[0],\n            bb[3] - bb[1],\n            linewidth=2,\n            edgecolor=color,\n            facecolor=\"none\",\n        )\n        ax.add_patch(bbox)\n        plt.text(\n            bb[0],\n            bb[0],\n            s=ycb[labels[i]],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": color, \"pad\": 0},\n        )\n    plt.axis(\"off\")\n\n\nplot_prediction()","block_group":"63cb7f26a7984dbeac2e383820626cf7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIfmykN-t7XG","cell_id":"1c08f01226d3451f92faea2a0ee633f3","deepnote_cell_type":"markdown"},"source":"# Visualize the region proposals \n\nLet's visualize some of the intermediate results of the networks.\n\nTODO: would be very cool to put a slider on this so that we could slide through ALL of the boxes.  But my matplotlib non-interactive backend makes it too tricky!","block_group":"ec08c72ee9b34cea9748ba6b57974770"},{"cell_type":"code","metadata":{"id":"zBNqFb68td8N","cell_id":"dbb5e98189fd463b9b1b06d930b8cea6","deepnote_cell_type":"code"},"source":"class Inspector:\n    \"\"\"A helper class from Kuni to be used for torch.nn.Module.register_forward_hook.\"\"\"\n\n    def __init__(self):\n        self.x = None\n\n    def hook(self, module, input, output):\n        self.x = output\n\n\ninspector = Inspector()\nmodel.rpn.register_forward_hook(inspector.hook)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nrpn_values = inspector.x\n\n\nimg_np = np.array(img)\nplt.figure()\nfig, ax = plt.subplots(1, figsize=(12, 9))\nax.imshow(img_np)\n\ncmap = plt.get_cmap(\"tab20b\")\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\nnum_to_draw = 20\nbbox_colors = random.sample(colors, num_to_draw)\nboxes = rpn_values[0][0].cpu().numpy()\nprint(\n    f\"Region proposals (drawing first {num_to_draw} out of {boxes.shape[0]})\"\n)\n\nfor i in range(num_to_draw):\n    color = bbox_colors[i]\n    bb = boxes[i, :]\n    bbox = patches.Rectangle(\n        (bb[0], bb[1]),\n        bb[2] - bb[0],\n        bb[3] - bb[1],\n        linewidth=2,\n        edgecolor=color,\n        facecolor=\"none\",\n    )\n    ax.add_patch(bbox)\nplt.axis(\"off\");","block_group":"7e5b2f8483ee4960a82e917398ebcb27","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"d22962f8a4d64049be834b05724cd230","deepnote_cell_type":"markdown"},"source":"# Try a few more images","block_group":"dee739a13f3b4e8785b08d5ccbd99a85"},{"cell_type":"code","metadata":{"cell_id":"fcd4826cc687477499ab5d85039efecd","deepnote_cell_type":"code"},"source":"# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9985)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nplot_prediction()","block_group":"3dfe4050f9454a059e03052995a99b1f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"9333b00944954d5d942199e354c9c038","deepnote_cell_type":"code"},"source":"","block_group":"e839a0b92ee4499c8848bd228479259b","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c91043b1-e029-4b16-9924-9f7aca75a010' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clutter_maskrcnn_inference.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.10.6 64-bit"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.8","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"f9806df3dd2b4d16998424a8020c056f","deepnote_execution_queue":[]}}