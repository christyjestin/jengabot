{"cells":[{"cell_type":"code","metadata":{"id":"lRBJuPiYOGgY","cell_id":"b9945f24ee5b41cdaf477c6bfcf24b9a","deepnote_cell_type":"code"},"source":"import random\nfrom matplotlib import pyplot as plt\n\nimport mpld3\nimport numpy as np\nimport torch\nfrom IPython.display import HTML, clear_output, display\nfrom scipy.signal import lfilter\nfrom torch import nn\nfrom torch.distributions.independent import Independent\nfrom torch.distributions.normal import Normal\nfrom torch.nn import functional as F\n\nfrom manipulation import running_as_notebook\nfrom manipulation.envs.planar_gripper_pushing_a_box import (\n    PlanarGripperPushingABoxEnv,\n)\nfrom manipulation.exercises.grader import Grader\nfrom manipulation.exercises.rl.test_vpg import TestVPG\n\n\nif running_as_notebook:\n    mpld3.enable_notebook()\n\n\ndef pad_to_last(nums, total_length, axis=-1, val=0):\n    \"\"\"Pad val to last in nums in given axis.\n\n    length of the result in given axis should be total_length.\n\n    Raises:\n      IndexError: If the input axis value is out of range of the nums array\n\n    Args:\n        nums (numpy.ndarray): The array to pad.\n        total_length (int): The final width of the Array.\n        axis (int): Axis along which a sum is performed.\n        val (int): The value to set the padded value.\n\n    Returns:\n        torch.Tensor: Padded array\n\n    \"\"\"\n    tensor = torch.Tensor(nums)\n    axis = (axis + len(tensor.shape)) if axis < 0 else axis\n\n    if len(tensor.shape) <= axis:\n        raise IndexError(\n            \"axis {} is out of range {}\".format(axis, tensor.shape)\n        )\n\n    padding_config = [0, 0] * len(tensor.shape)\n    padding_idx = abs(axis - len(tensor.shape)) * 2 - 1\n    padding_config[padding_idx] = max(total_length - tensor.shape[axis], val)\n    return F.pad(tensor, padding_config)\n\n\ndef filter_valids(tensor, valids):\n    \"\"\"Filter out tensor using valids (last index of valid tensors).\n\n    valids contains last indices of each rows.\n\n    Args:\n        tensor (torch.Tensor): The tensor to filter\n        valids (list[int]): Array of length of the valid values\n\n    Returns:\n        torch.Tensor: Filtered Tensor\n\n    \"\"\"\n    return [tensor[i][:valid] for i, valid in enumerate(valids)]\n\n\ndef discount_cumsum(x, discount):\n    \"\"\"Discounted cumulative sum.\n\n    See https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html#difference-equation-filtering  # noqa: E501\n    Here, we have y[t] - discount*y[t+1] = x[t]\n    or rev(y)[t] - discount*rev(y)[t-1] = rev(x)[t]\n\n    Args:\n        x (np.ndarrary): Input.\n        discount (float): Discount factor.\n\n    Returns:\n        np.ndarrary: Discounted cumulative sum.\n\n\n    \"\"\"\n    return lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n\n\nclass BatchDataset:\n    def __init__(self, inputs, batch_size, extra_inputs=None):\n        self._inputs = [i for i in inputs]\n        if extra_inputs is None:\n            extra_inputs = []\n        self._extra_inputs = extra_inputs\n        self._batch_size = batch_size\n        if batch_size is not None:\n            self._ids = np.arange(self._inputs[0].shape[0])\n            self.update()\n\n    @property\n    def number_batches(self):\n        if self._batch_size is None:\n            return 1\n        return int(np.ceil(self._inputs[0].shape[0] * 1.0 / self._batch_size))\n\n    def iterate(self, update=True):\n        if self._batch_size is None:\n            yield list(self._inputs) + list(self._extra_inputs)\n        else:\n            for itr in range(self.number_batches):\n                batch_start = itr * self._batch_size\n                batch_end = (itr + 1) * self._batch_size\n                batch_ids = self._ids[batch_start:batch_end]\n                batch = [d[batch_ids] for d in self._inputs]\n                yield list(batch) + list(self._extra_inputs)\n            if update:\n                self.update()\n\n    def update(self):\n        np.random.shuffle(self._ids)\n\n\nclass MLPGaussian(nn.Module):\n    def __init__(\n        self,\n        input_dim=7,\n        output_dim=6,\n        num_hidden=2,\n        hidden_dim=128,\n        nonlinear_act=nn.ReLU,\n        hidden_w_init=nn.init.xavier_normal_,\n        hidden_b_init=nn.init.zeros_,\n        output_w_inits=nn.init.xavier_normal_,\n        output_b_inits=nn.init.zeros_,\n    ):\n        super(MLPGaussian, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.fc_in = nn.Linear(input_dim, hidden_dim)\n        self.act = nonlinear_act()\n        self.fc_list = nn.ModuleList()\n        for i in range(num_hidden - 1):\n            hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n            hidden_w_init(hidden_layer.weight)\n            hidden_b_init(hidden_layer.bias)\n            self.fc_list.append(hidden_layer)\n\n        self.fc_out_mean = nn.Linear(hidden_dim, output_dim)\n        self.fc_out_var = nn.Linear(hidden_dim, output_dim)\n\n        output_w_inits(self.fc_out_mean.weight)\n        output_b_inits(self.fc_out_mean.bias)\n\n    def forward(self, x):\n        # out = x.view(-1, self.input_dim)\n        out = self.fc_in(x)\n        out = self.act(out)\n        for _, layer in enumerate(self.fc_list, start=0):\n            out = layer(out)\n            out = self.act(out)\n        out_mean = self.fc_out_mean(out)\n        out_var = self.fc_out_var(out)\n        out_var = self.act(out_var)\n        out_var = (\n            out_var + 0.001\n        )  # add a small bias to make sure it is not equal to 0\n        # return (out_mean, out_var)\n        return out_mean\n\n\ndef init_path():\n    path_sample = {\n        \"observations\": [],\n        \"next_observations\": [],\n        \"actions\": [],\n        \"rewards\": [],\n        \"infos\": [],\n    }\n    return path_sample\n\n\nclass REINFORCE:\n    def __init__(\n        self,\n        env,\n        policy,\n        value_function,\n        util_compute_policy_loss,\n        util_compute_value_loss,\n        util_compute_advantage,\n        use_advantage=False,\n        gae_lambda=1.0,\n        max_episode_length=100,\n        discount_ratio=0.99,\n        learning_rate=0.01,\n    ):\n        self.env = env\n        self.max_episode_length = max_episode_length\n        self.discount = discount_ratio\n        self._gae_lambda = gae_lambda\n        self.util_compute_policy_loss = util_compute_policy_loss\n        self.util_compute_value_loss = util_compute_value_loss\n        self.util_compute_advantage = util_compute_advantage\n        self.use_advantage = use_advantage\n\n        self.policy = policy\n        self._value_function = value_function\n        self._policy_optimizer = torch.optim.Adam(\n            self.policy.parameters(), lr=learning_rate\n        )\n        self._vf_optimizer = torch.optim.Adam(\n            self._value_function.parameters(), lr=learning_rate\n        )\n\n    def _process_samples(self, paths):\n        r\"\"\"Process sample data based on the collected paths.\n\n        Notes: P is the maximum episode length (self.max_episode_length)\n\n        Args:\n            paths (list[dict]): A list of collected paths\n\n        Returns:\n            torch.Tensor: The observations of the environment\n                with shape :math:`(N, P, O*)`.\n            torch.Tensor: The actions fed to the environment\n                with shape :math:`(N, P, A*)`.\n            torch.Tensor: The acquired rewards with shape :math:`(N, P)`.\n            list[int]: Numbers of valid steps in each paths.\n            torch.Tensor: Value function estimation at each step\n                with shape :math:`(N, P)`.\n\n        \"\"\"\n        valids = torch.Tensor([len(path[\"actions\"]) for path in paths]).int()\n        obs = torch.stack(\n            [\n                pad_to_last(\n                    path[\"observations\"],\n                    total_length=self.max_episode_length,\n                    axis=0,\n                )\n                for path in paths\n            ]\n        )\n        actions = torch.stack(\n            [\n                pad_to_last(\n                    path[\"actions\"],\n                    total_length=self.max_episode_length,\n                    axis=0,\n                )\n                for path in paths\n            ]\n        )\n        rewards = torch.stack(\n            [\n                pad_to_last(\n                    path[\"rewards\"], total_length=self.max_episode_length\n                )\n                for path in paths\n            ]\n        )\n        returns = torch.stack(\n            [\n                pad_to_last(\n                    discount_cumsum(path[\"rewards\"], self.discount).copy(),\n                    total_length=self.max_episode_length,\n                )\n                for path in paths\n            ]\n        )\n        with torch.no_grad():\n            baselines = self._value_function(obs)\n\n        return obs, actions, rewards, returns, valids, baselines\n\n    def collect_paths(self, batch_size=100, max_episode_length=100):\n        \"\"\"\n        Args:\n            env: simulation environment (e.g.Gym)\n            batch_size (int, optional): the number of episodes to be included in one patch.\n                                        Defaults to 100.\n            max_episode_length (int, optional): the maximum episode length\n        Returns:\n            [list]: a list of dicts, each dict stores data of one episode\n        \"\"\"\n        env = self.env\n        paths = []\n        invalid_episode_count = 0\n        while len(paths) < batch_size:\n            observation, *_ = env.reset()\n            this_path = init_path()\n            good_episode = True\n\n            for t in range(max_episode_length):\n                # take a step\n                action, _ = self.policy.get_action(observation)\n                observation2, reward, done, truncated, info = env.step(action)\n                if not truncated:\n                    # env.render()\n                    this_path[\"observations\"].append(observation)\n                    this_path[\"next_observations\"].append(observation2)\n                    this_path[\"actions\"].append(action)\n                    this_path[\"rewards\"].append(reward)\n                    this_path[\"infos\"].append(info)\n                else:\n                    # discard this episode if the\n                    # simulation/integration fails\n                    # this is because the time step\n                    # in our simulation\n                    # plant's  is considerably\n                    # large (0.01), in order to\n                    # speed up simulation.\n                    good_episode = False\n                    invalid_episode_count += 1\n                    break\n\n                if done:\n                    break\n                observation = observation2\n            if good_episode:\n                for key in this_path.keys():\n                    this_path[key] = np.c_[this_path[key]]\n                this_path[\"rewards\"] = this_path[\"rewards\"].squeeze()\n                paths.append(this_path)\n        return paths\n\n    def get_mini_batch_data(self, *inputs):\n        \"\"\"mini batch training data generator\"\"\"\n        batch_dataset = BatchDataset(inputs, None)\n        for _ in range(1):\n            for dataset in batch_dataset.iterate():\n                yield dataset\n\n    def train_from_episode_batch(self, itr, paths):\n        \"\"\"Train the algorithm from an episode batch\n\n        Args:\n            itr (int): Iteration number.\n            paths (list[dict]): A list of collected paths.\n\n        Returns:\n            numpy.float64: Calculated mean value of undiscounted returns.\n\n        \"\"\"\n        # the individual paths are not of the same length,\n        # the length of the paths are stored in valids\n\n        (\n            obs,\n            actions,\n            rewards,\n            returns,\n            valids,\n            baselines,\n        ) = self._process_samples(paths)\n        obs_flat = torch.cat(filter_valids(obs, valids))\n        actions_flat = torch.cat(filter_valids(actions, valids))\n        rewards_flat = torch.cat(filter_valids(rewards, valids))\n        returns_flat = torch.cat(filter_valids(returns, valids))\n        advs_flat = self._compute_advantage(rewards, valids, baselines)\n\n        value_loss, policy_loss = 0.0, 0.0\n        if self.use_advantage:\n            for dataset in self.get_mini_batch_data(\n                obs_flat, actions_flat, advs_flat\n            ):\n                policy_loss = self._train_policy(*dataset)\n        else:\n            for dataset in self.get_mini_batch_data(\n                obs_flat, actions_flat, returns_flat\n            ):\n                policy_loss = self._train_policy(*dataset)\n\n        for dataset in self.get_mini_batch_data(obs_flat, returns_flat):\n            value_loss = self._train_value_function(*dataset)\n\n        mean_episode_lengths = np.mean(valids.numpy())\n        # undiscounted_returns = list(torch.sum(rewards, dim=1).numpy())\n\n        total_reward = float(float(rewards_flat.sum()))\n        policy_loss, value_loss = policy_loss.item(), value_loss.item()\n\n        print(\n            \"{}, total reward:{}, policy_loss:{}, value_loss:{}, mean episode length:{}\".format(\n                itr,\n                total_reward,\n                policy_loss,\n                value_loss,\n                mean_episode_lengths,\n            )\n        )\n\n        return total_reward, mean_episode_lengths, policy_loss, value_loss\n\n    def _train_policy(self, obs, actions, returns):\n        self._policy_optimizer.zero_grad()\n        loss = self.compute_policy_loss(obs, actions, returns)\n        loss.backward()\n        self._policy_optimizer.step()\n        return loss\n\n    def _train_value_function(self, obs, returns):\n        self._vf_optimizer.zero_grad()\n        loss = self.util_compute_value_loss(self._value_function, obs, returns)\n        loss.backward()\n        self._vf_optimizer.step()\n        return loss\n\n    def compute_policy_loss(self, obs, actions, returns):\n        \"\"\"call student's implementation\"\"\"\n        loss = self.util_compute_policy_loss(\n            self.policy, obs, actions, returns\n        )\n        return loss\n\n    def _compute_advantage(self, rewards, valids, baselines):\n        \"\"\"compute the advantage function\n\n        Args:\n            rewards (torch.tensor): the episodic rewards, batch_size x max episode length\n            valids (torch.tensor): valid episodic lengths\n                    to allow concatenation of episodes of different lengths,\n                    by default, the max episode length is used instead of\n                    the actual episode lengths to construct reward and\n                    baseline tensors\n            baselines (torch.tensor): estimated baselines (state values)\n                                      batch_size x max episode length\n\n        Returns:\n            [torch.tensor]: flattened advantage function\n        \"\"\"\n        advantages = self.util_compute_advantage(\n            self.discount,\n            self._gae_lambda,\n            self.max_episode_length,\n            baselines,\n            rewards,\n        )\n        advantage_flat = torch.cat(filter_valids(advantages, valids))\n\n        return advantage_flat\n\n\ndef draw_training_stats(training_stats, title):\n    plt.rcParams.update({\"font.size\": 12})\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(\n        nrows=4, ncols=1, sharex=True, figsize=(10, 10)\n    )\n    axes = [ax1, ax2, ax3, ax4]\n    ax1.plot(training_stats[\"total_reward\"], \"o-\", label=\"total_rewards\")\n    ax2.plot(training_stats[\"policy_loss\"], \"o-\", label=\"policy_loss\")\n    ax3.plot(training_stats[\"value_loss\"], \"o-\", label=\"value_loss\")\n    ax4.plot(\n        training_stats[\"avg_episode_length\"], \"o-\", label=\"avg_episode_length\"\n    )\n    ax1.set_title(title)\n    for ax in axes:\n        ax.legend()\n    axes[-1].set_xlabel(\"# of episodes\")\n\n    ax3.set_yscale(\"log\")\n    ax2.set_yscale(\"log\")\n\n\ndef visualize_policy(reward_function, policy=None):\n    if running_as_notebook:\n        env = PlanarGripperPushingABoxEnv(\n            render=True, reward_function=reward_function\n        )\n        observation, *_ = env.reset()\n        vis = env.simulator.get_system().GetSubsystemByName(\"visualizer\")\n        vis.start_recording()\n        for i in range(10):\n            action = None\n            if policy is None:\n                action = (\n                    np.random.rand(\n                        3,\n                    )\n                    * 0.1\n                    - 0.05\n                )\n            else:\n                action, _ = algo.policy.get_action(observation)\n            observation, *_ = env.step(action)\n\n        vis.stop_recording()\n        ani = vis.get_recording_as_animation(repeat=True)\n        display(HTML(ani.to_jshtml()))","block_group":"95f27f7e644c40aabc0dfad46a9d12c8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNUFdbcpOGgY","cell_id":"d3c2435821064a5cb4a5f18803050097","deepnote_cell_type":"markdown"},"source":"# REINFORCE\nIn the lecture, you have been introduced to the idea of policy gradient, where the objective is to find a direct mapping from states to actions. In this exercise, you will implement REINFORCE, also commonly referred to as Vanilla Policy Gradient (VPG) method, to solve simple manipulation problems. To finish this exercise, you need to complete the 3 steps listed below:\n\n1. Implement the method to compute the loss of the policy function\n2. Implement the method to compute the loss of the value function\n3. Implement the method to compute the advantage function","block_group":"7564fff8532a430090ff08c7213eec95"},{"cell_type":"markdown","metadata":{"id":"Q7aRTpchOGgY","cell_id":"5454d8c68947457c8b34143812e734ae","deepnote_cell_type":"markdown"},"source":"Now let's check out the manipulation problem you are going to solve! The objective of the problem is to manipulate the foam block in the scene to maximize its $x$ coordinate. We intentionally make this problem extremely simple to help you train and debug faster. If you would like to solve a more difficult task, you are welcome to modify the reward function defined in the cell below. Note that since REINFORCE is an on-policy method, it is not known for being data efficient, so expect longer training time depending on how difficult your task is.","block_group":"7355455e686f48189f825dbc1473dabe"},{"cell_type":"code","metadata":{"id":"i9pqcLB3OGgY","colab":{"height":533,"base_uri":"https://localhost:8080/"},"outputId":"afa1e2bf-ee34-4aaa-a880-a473fa281748","cell_id":"e2c3589d17e44a439303f4d4ea41472c","deepnote_cell_type":"code"},"source":"def reward_function(system, context):\n    observation = system.GetOutputPort(\"position\").Eval(context)\n    # observation = [block_x, block_z, block_theta,\n    #                gripper_x, gripper_z, gripper_theta]\n\n    # compute dense rewards\n\n    reward = observation[0]\n    if observation[0] > 0.1:\n        reward = 100.0  # reward goal completion\n    return reward\n\n\nvisualize_policy(reward_function, None)","block_group":"ca7045b6e7f94035a61be4a96d10f5ca","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4t08PqPA3U9","cell_id":"e8dc157da3e247ba9e1db28f20a9a897","deepnote_cell_type":"markdown"},"source":"In this exercise, we use the Markov Decision Process (MDP) formulation. We assume we have the full access to the poses of the gripper and foam block. In the following sections, we use states and observations interchangeably, both referring to the concatenation of block pose and gripper pose.","block_group":"a80b40e35a1949d5ae974bb2fcdff513"},{"cell_type":"markdown","metadata":{"id":"zzN6kklYOGga","cell_id":"504e15efc59b42ec9c130115b198f292","deepnote_cell_type":"markdown"},"source":"## Review of REINFORCE Algorithm\n\n**REINFORCE** (Monte-Carlo policy gradient) relies on an estimated return by Monte-Carlo methods using episode samples to update the policy parameter $\\theta$. REINFORCE works because the expectation of the sample gradient is equal to the actual gradient:\n\n$$\n\\nabla_\\theta J(\\theta)  = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\ln \\pi_\\theta (x_t, u_t) Q^{\\pi_\\theta}(x_t, u_t) ]\n$$\n\n\nwhere $\\pi_\\theta$ is the policy parameterized by $\\theta$, and the action distribution computed from the policy is $\\pi_\\theta(u_t, x_t) = p_{\\pi_\\theta}(u_t \\vert x_t)$. Since $Q^{\\pi_\\theta}(x_t, u_t)$ is unknown, we instead use unbiased samples to approximate it\n\n$$ Q^\\pi(x_t, u_t) = \\mathbb{E}_\\pi[G_t \\vert x_t, u_t] $$\n\nwhere $G_t$ is the returns computed from samples\n\n\nTherefore we can measure $G_t$ from real sample trajectories and use that to update our policy gradient. It relies on a full trajectory and that's why it is a Monte-Carlo method.\n\nThe REINFORCE algorithm is quite straightforward:\n\nInitialize the policy parameter $\\theta$ at random.\n\nGenerate one trajectory on policy $\\pi_\\theta: x_1, u_1, r_2, x_2, u_2, \\dots, x_t$\n\nFor t=1, 2, ..., T:\n\n1. Estimate the the return $G_t= \\sum_{t=0}^{T} \\alpha^t R_t$\n\n2. Update policy parameters: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi_\\theta(x_t, u_t)G_t$, where $\\alpha$ is the learning rate\n\n\nA widely used variation of REINFORCE is to subtract a baseline value from the return $G_t$ to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible).","block_group":"09b126f2659f431f974de88a26ecfe4e"},{"cell_type":"markdown","metadata":{"id":"N7QF_hV7OGga","cell_id":"7d643cb7ae324cfa9413657468b2469c","deepnote_cell_type":"markdown"},"source":"To implement REINFORCE algorithm to solve continuous control tasks, you will inevitably need function approximators to estimate the policy distribution and optionally value function if an advantage function is used instead of the total discounted rewards for $G_t$. To help you quickly get to the core of the algorithm, we have provided you the implementation of the majority parts of the algorithm. You may find these codes in the `REINFORCE`, `PolicyEstimator`, `ValueEstimator` classes from setup cell. Note that for this exercise, you are not required to fully understand these codes.","block_group":"8e44df3eaf6a4b4ab696c10f5f4b29b7"},{"cell_type":"code","metadata":{"id":"PufvcPKIOGga","cell_id":"7fa03d2ef1e74e1c87eefe6510f2e4ec","deepnote_cell_type":"code"},"source":"class PolicyEstimator(nn.Module):\n    def __init__(\n        self,\n        num_hidden,\n        hidden_dim,\n        obs_dim=None,\n        action_dim=None,\n        nonlinear_act=nn.Tanh,\n        init_std=1.0,\n    ):\n        super(PolicyEstimator, self).__init__()\n        self._obs_dim = obs_dim\n        self._action_dim = action_dim\n\n        self.num_hidden = num_hidden\n        self.hidden_dim = hidden_dim\n        self.model = MLPGaussian(\n            input_dim=self._obs_dim,\n            output_dim=self._action_dim,\n            num_hidden=num_hidden,\n            hidden_dim=hidden_dim,\n            nonlinear_act=nonlinear_act,\n        )\n\n        init_std_param = torch.Tensor([init_std]).log()\n        self._init_log_std = torch.nn.Parameter(init_std_param)\n\n    def predict(self, state):\n        state_tensor = state\n        if isinstance(state, np.ndarray):\n            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n        action_mean = self.model(state_tensor)\n        # post process the action mean\n        # action_mean = torch.sigmoid(action_mean)*0.005 - 0.0025\n        action_std = self._init_log_std.exp() * torch.ones_like(action_mean)\n        action_distribution = Normal(action_mean, action_std)\n        return Independent(action_distribution, 1)\n\n    def forward(self, observations):\n        dist = self.predict(observations)\n        return (\n            dist,\n            dict(mean=dist.mean, log_std=(dist.variance**0.5).log()),\n        )\n        # return dist\n\n    def get_action(self, observation):\n        r\"\"\"Get a single action given an observation.\n\n        Args:\n            observation (np.ndarray): Observation from the environment.\n                Shape is :math:`env_spec.observation_space`.\n\n        Returns:\n            tuple:\n                * np.ndarray: Predicted action. Shape is\n                    :math:`env_spec.action_space`.\n                * dict:\n                    * np.ndarray[float]: Mean of the distribution\n                    * np.ndarray[float]: Standard deviation of logarithmic\n                        values of the distribution.\n        \"\"\"\n        if not isinstance(observation, np.ndarray) and not isinstance(\n            observation, torch.Tensor\n        ):\n            observation = self._env_spec.observation_space.flatten(observation)\n        with torch.no_grad():\n            if not isinstance(observation, torch.Tensor):\n                observation = torch.as_tensor(observation).float()\n            observation = observation.unsqueeze(0)\n            action, agent_infos = self.get_actions(observation)\n            return action[0], {k: v[0] for k, v in agent_infos.items()}\n\n    def get_actions(self, observations):\n        r\"\"\"Get actions given observations.\n\n        Args:\n            observations (np.ndarray): Observations from the environment.\n                Shape is :math:`batch_dim \\bullet env_spec.observation_space`.\n\n        Returns:\n            tuple:\n                * np.ndarray: Predicted actions.\n                    :math:`batch_dim \\bullet env_spec.action_space`.\n                * dict:\n                    * np.ndarray[float]: Mean of the distribution.\n                    * np.ndarray[float]: Standard deviation of logarithmic\n                        values of the distribution.\n        \"\"\"\n        if isinstance(observations, list):\n            if isinstance(observations[0], np.ndarray):\n                observations = np.stack(observations)\n            elif isinstance(observations[0], torch.Tensor):\n                observations = torch.stack(observations)\n        with torch.no_grad():\n            if not isinstance(observations, torch.Tensor):\n                observations = torch.as_tensor(observations).float().to()\n            dist, info = self.forward(observations)\n            return dist.sample().cpu().numpy(), {\n                k: v.detach().cpu().numpy() for (k, v) in info.items()\n            }\n\n    def compute_log_likelihood(self, obs, actions):\n        \"\"\"\n        Args:\n            obs (torch.tensor): flattened observations\n            actions (torch.tensor): flattened actions\n        return\n            log_likelihoods (torch.tensor): log probabilities of\n                                          sampling the selected actions\n        \"\"\"\n        distribution = self.predict(obs)\n        return distribution.log_prob(actions)\n\n\nclass ValueEstimator(nn.Module):\n    def __init__(\n        self,\n        num_hidden,\n        hidden_dim,\n        obs_dim=None,\n        action_dim=None,\n        nonlinear_act=nn.Tanh,\n        init_std=1.0,\n    ):\n        super(ValueEstimator, self).__init__()\n        self._obs_dim = obs_dim\n        self._action_dim = action_dim\n        self.num_hidden = num_hidden\n        self.hidden_dim = hidden_dim\n        self.model = MLPGaussian(\n            input_dim=self._obs_dim,\n            output_dim=1,\n            num_hidden=num_hidden,\n            hidden_dim=hidden_dim,\n            nonlinear_act=nonlinear_act,\n        )\n\n        init_std_param = torch.Tensor([init_std]).log()\n        self._init_log_std = torch.nn.Parameter(init_std_param)\n        self.module = self.model\n\n    def predict(self, state):\n        state_tensor = state\n        if isinstance(state, np.ndarray):\n            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n        value_mean = self.model(state_tensor)\n        value_std = self._init_log_std.exp() * torch.ones_like(value_mean)\n        value_distribution = Normal(value_mean, value_std)\n        return Independent(value_distribution, 1)\n\n    def compute_log_likelihood(self, obs, returns):\n        \"\"\"\n        Args:\n            obs (torch.tensor): flattened observations\n            returns (torch.tensor): flattened a measure of gain\n                                    (e.g.advantage function or discounted rewards)\n        return\n            log_likelihoods (torch.tensor): log probabilities of\n                                          sampling the selected actions\n        \"\"\"\n        dist = self.predict(obs)\n        log_likelihoods = dist.log_prob(returns.reshape(-1, 1))\n        return log_likelihoods\n\n    def forward(self, obs):\n        # torch.Size([246, 1000, 4])\n        dist = self.predict(obs)\n        return dist.mean.flatten(-2)","block_group":"77077c3a7650453bb27ef78ec7fc78c0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SeZ6Wm3EOGga","cell_id":"d8122679313545ebb4dfdeb677fbc1a0","deepnote_cell_type":"markdown"},"source":"### (a) Compute Policy Loss\n\nFrom the previous section, as for training the policy model, we would like to perform gradient ascent\n\n$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi_\\theta(x_t, u_t)G_t$$\n\nThanks to the auto-differentiation feature from PyTorch, we only need to compute the objective value of $ j_t^{\\pi} = \\ln \\pi_\\theta(x_t, u_t)G_t $. From there, simply following the procedure below will automatically compute gradients to optimize all relevant parameters.\n\n```\noptimizer.zero_grad()\nloss = compute_loss(...) # we are at this step\nloss.backward()\noptimizer.step() # by default, minimize loss\n```\n\nEvaluating objective values in batch can effectively reduce the variance in the computed gradients.\n\n$$ J_t^{\\pi} \\leftarrow \\frac{\\sum_{i=1}^N \\ln \\pi_\\theta(x_t, u_t)G_t}{N} $$\n\nwhere $\\ln \\pi_\\theta(x_t, u_t)$ can be obtained from the supplied method `policy.compute_log_likelihood`.\n\nBy default, the PyTorch optimizer minimizes a scalar *loss*. Since in our case we would like to increase the log-likelihoods of generating better actions, the associated loss is\n$$ L_t^{\\pi} \\leftarrow - J_t^{\\pi}  $$\n\nNow in the cell below, implement the `util_compute_policy_loss(policy, obs, actions, returns)`. Note that `obs`, `actions`, `returns` are all pre-flattened. You may treat them as data in a batch.\n","block_group":"10fa8017fe1747d68e5aa5e2c4ca54e1"},{"cell_type":"code","metadata":{"id":"RGpvJHitOGga","cell_id":"19b59b2b9881430db6dc85cc52ecc4e6","deepnote_cell_type":"code"},"source":"def util_compute_policy_loss(policy, obs, actions, returns):\n    \"\"\"compute policy loss given observations, actions,\n       and advantages (or total discounted rewards)\n       Note: One can either use the advantage function\n             or discounted returns\n\n    Args:\n        policy (PolicyEstimator): a policy instance\n        obs (torch.tensor): flattened observations Size: data_size, observation dim\n        actions (torch.tensor): flattened actions Size: data_size, action_dim\n        returns (torch.tensor): flattened total discounted returns. Size: data_size\n        advantages (Optional, torch.tensor): flattened advantages\n                                             advantages are not needed here\n\n    Returns:\n        torch.tensor: the mean policy loss, should have size of 1\n    \"\"\"\n    loss = torch.tensor([0.0])\n    return loss","block_group":"5313b9f272fb43d78ac223d8340f36bd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heI_WdygPaEz","cell_id":"f91731b9062a434ab488c44b93dd5c2c","deepnote_cell_type":"markdown"},"source":"### (b) Compute Value Loss\n\nWith only the policy loss computed above, you may already run the REINFORCE algorithm as you will see later. To reduce the variance of gradients further, another useful trick is to replace the discounted total rewards with an advantage function $A^{\\pi}$. In this case, the update rule is then\n\n$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\ln \\pi_\\theta(x_t, u_t)u_t^{\\pi}$$\n\nAn advantage function intuitively compares the *advantage* of a particular action compared to an *average* action. The value of a particular station-action pair is $Q(x_t, u_t)$, whereas the value of an average action at state $x_t$ is $V(x_t)$. $V(x_t)$ is the value function of the Markov Decision Process (MDP). The estimated state values are also referred to as the baselines.\n\nNow our job is to learn a value function of the MDP problem. It should map from states to scalar values that estimate the average future rewards that can be received from the designated states.\n\n\nLike in part (a), you only need to implement the loss function (not the gradient step itself). You're given the returns (which tells you the training target $V_t$), the observations, and value function, which can give you the log likelihood of the training target given the parameters and other inputs. You want to choose the parameters $\\theta$ that maximizes the log likelihood of the training target:\n$$ \\max_\\theta \\, \\ln p(V_t \\vert u_t, \\theta) $$\nwhere $\\theta$ here refers to the parameters of the value function, and $V_t$ is the training targets stored in `returns`.\n\n**Follow similar steps as in the policy loss, complete the `util_compute_value_loss(value_function, obs, returns)` below.** Note that you want to return the mean loss accross all observations.","block_group":"6515a7b71ea94ac38bccfa7513912959"},{"cell_type":"code","metadata":{"id":"Mk93nxDqOGga","cell_id":"3ab19eca43ce4a9ba70a51e59992eb4d","deepnote_cell_type":"code"},"source":"def util_compute_value_loss(value_function, obs, returns):\n    \"\"\"compute value function loss given observations and returns\n       Note: One can either use the advantage function\n             or discounted returns\n\n    Args:\n        value_function (ValueEstimator): an instance of a value function\n        obs (torch.tensor): flattened observations\n        returns (torch.tensor): total discounted returns\n\n    Returns:\n        torch.tensor: the mean policy loss, should have size of 1\n    \"\"\"\n    loss = torch.tensor([0.0])\n    return loss","block_group":"979adf5b9d634a98b0b238ad2b9fb04a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nilLtBgVPkDg","cell_id":"d8522ab6caad44429070660f7e8f66be","deepnote_cell_type":"markdown"},"source":"### (c) Compute Advantages\n\nIn this part, we will implement the computation of an advantage function.\n\nMathematically, we can evaluate the *advantage* of a selected action $u_t$ at $x_t$ by\n\n$$\\delta_t = Q^\\pi(x_t, u_t) - V^\\pi(x_t) =  R(x_t) + \\gamma V'(x_{t+1}) - V'(x_t)$$\n\nwhere $V'(x_t), V'(x_{t+1})$ are evaluated by the value function learned above, $\\gamma$ is the discount factor.\n\nNote $\\delta_t$ is the advantage for a time step. For a trajectory of states and actions, we can compute the discounted sum of advantages similar to rewards.\n\n$$A_t = \\sum_{k=0}^{T-t-1}(\\lambda\\gamma)^k \\delta_{t+k}$$\n\n**In the cell below, compute the advantage function by completing the `compute_advantages` method.**\n\nNote: the baselines are the estimated state values. The rewards are raw rewards, NOT the discounted sums. The shape of the rewards and baselines are both $N \\times T$. This is because an episode may terminate in fewer steps than the maximum episode length. The elements after the termination of episodes are set to 0.","block_group":"56edfd864e224bfb8db878141b4bd4e5"},{"cell_type":"code","metadata":{"id":"CYKlkNrXOGgb","cell_id":"8ffc621d88614b74b80044bf0dadebc8","deepnote_cell_type":"code"},"source":"def compute_advantages(\n    discount, gae_lambda, max_episode_length, baselines, rewards\n):\n    \"\"\"Calculate advantages.\n\n    Advantages are a discounted cumulative sum.\n\n    Calculate advantages using a baseline according to Generalized Advantage\n    Estimation (GAE)\n\n    Args:\n        discount (float): RL discount factor (i.e. gamma).\n        gae_lambda (float): Lambda, as used for Generalized Advantage\n            Estimation (GAE).\n        max_episode_length (int): Maximum length of a single episode.\n        baselines (torch.Tensor): A 2D vector of value function estimates with\n            shape (N, T), where N is the batch dimension (number of episodes)\n            and T is the maximum episode length experienced by the agent. If an\n            episode terminates in fewer than T time steps, the remaining\n            elements in that episode should be set to 0.\n        rewards (torch.Tensor): A 2D vector of per-step rewards with shape\n            (N, T), where N is the batch dimension (number of episodes) and T\n            is the maximum episode length experienced by the agent. If an\n            episode terminates in fewer than T time steps, the remaining\n            elements in that episode should be set to 0.\n\n    Returns:\n        torch.Tensor: A 2D vector of calculated advantage values with shape\n            (N, T), where N is the batch dimension (number of episodes) and T\n            is the maximum episode length experienced by the agent. If an\n            episode terminates in fewer than T time steps, the remaining values\n            in that episode should be set to 0.\n\n    \"\"\"\n    advantages = torch.zeros_like(rewards)\n    return advantages","block_group":"fd7e7fdff3564772b8baa47b28e1e8ef","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfZgOm1qOGgb","cell_id":"0dfe38bef6194f28bd29fa339d1527b8","deepnote_cell_type":"code"},"source":"# make everything deterministic\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nenv = PlanarGripperPushingABoxEnv(\n    render=False, reward_function=reward_function\n)\naction_dim = 3\nobs_dim = 6","block_group":"1398668d29f1465aa925775919a293dd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMi53IATR37n","cell_id":"a0f341f71354447383554333a7e14102","deepnote_cell_type":"markdown"},"source":"### REINFORCE without Advantage Function\n\nNow we can compare the performance of the REINFORCE algorithm with and without using the advantage function.","block_group":"f7468a7f8a984cbba3c5dd20615f4acc"},{"cell_type":"code","metadata":{"id":"s43GiJqzOGgb","colab":{"height":741,"base_uri":"https://localhost:8080/"},"outputId":"ca09b51a-9b7f-40e0-9a22-56d4f63e5b71","cell_id":"de7fb2caece44531aab51398fae280d4","deepnote_cell_type":"code"},"source":"policy = PolicyEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\nvalue_function = ValueEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\nuse_advantage = False\nprint(\"using advantage: {}\".format(use_advantage))\nalgo = REINFORCE(\n    env=env,\n    policy=policy,\n    value_function=value_function,\n    util_compute_policy_loss=util_compute_policy_loss,\n    util_compute_value_loss=util_compute_value_loss,\n    util_compute_advantage=compute_advantages,\n    use_advantage=use_advantage,\n)\nprint(\"start training\")\ntraining_stats = {\n    \"total_reward\": [],\n    \"policy_loss\": [],\n    \"value_loss\": [],\n    \"avg_episode_length\": [],\n}\nnum_episodes = 50\n\nif running_as_notebook:\n    for i in range(num_episodes):\n        paths = algo.collect_paths(\n            batch_size=20,\n            max_episode_length=10,\n        )\n        (\n            total_reward,\n            mean_episode_lengths,\n            policy_loss,\n            value_loss,\n        ) = algo.train_from_episode_batch(i, paths)\n\n        training_stats[\"total_reward\"].append(total_reward)\n        training_stats[\"policy_loss\"].append(policy_loss)\n        training_stats[\"value_loss\"].append(value_loss)\n        training_stats[\"avg_episode_length\"].append(mean_episode_lengths)\n\n    clear_output()\n    draw_training_stats(training_stats, \"training_report\")","block_group":"87867dd0d1e54100813f7d92fb3bb580","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5BruT_OOGgb","colab":{"height":533,"base_uri":"https://localhost:8080/"},"outputId":"8e2b033d-ed83-42f4-e8b7-3b80ed6baa69","cell_id":"197b775df9564102b635f3a822be7963","deepnote_cell_type":"code"},"source":"visualize_policy(reward_function, policy)","block_group":"bc14e2f9a3dc4751b9ea01f6ac5c117c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ljWLRsFJR9nq","cell_id":"cba7ce7f3b3743bd963db8e044f2cfc5","deepnote_cell_type":"markdown"},"source":"### REINFORCE with Advantage Function","block_group":"905a3fd250df421e898d105a8c1b7e69"},{"cell_type":"code","metadata":{"id":"g89Mq8r8OGgc","colab":{"height":741,"base_uri":"https://localhost:8080/"},"outputId":"70861381-29aa-40a7-b194-58d51f53d961","cell_id":"fdf78e6cad8d4fd38d145c9963e4c251","deepnote_cell_type":"code"},"source":"policy = PolicyEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\nvalue_function = ValueEstimator(\n    action_dim=action_dim,\n    obs_dim=obs_dim,\n    num_hidden=2,\n    hidden_dim=64,\n    nonlinear_act=nn.ReLU,\n)\n\nuse_advantage = True\nprint(\"using advantage: {}\".format(use_advantage))\nalgo = REINFORCE(\n    env=env,\n    policy=policy,\n    value_function=value_function,\n    util_compute_policy_loss=util_compute_policy_loss,\n    util_compute_value_loss=util_compute_value_loss,\n    util_compute_advantage=compute_advantages,\n    use_advantage=use_advantage,\n)\nprint(\"start training\")\ntraining_stats = {\n    \"total_reward\": [],\n    \"policy_loss\": [],\n    \"value_loss\": [],\n    \"avg_episode_length\": [],\n}\nnum_episodes = 50\n\nif running_as_notebook:\n    for i in range(num_episodes):\n        paths = algo.collect_paths(\n            batch_size=20,\n            max_episode_length=10,\n        )\n        (\n            total_reward,\n            mean_episode_lengths,\n            policy_loss,\n            value_loss,\n        ) = algo.train_from_episode_batch(i, paths)\n\n        training_stats[\"total_reward\"].append(total_reward)\n        training_stats[\"policy_loss\"].append(policy_loss)\n        training_stats[\"value_loss\"].append(value_loss)\n        training_stats[\"avg_episode_length\"].append(mean_episode_lengths)\n\n    clear_output()\n    draw_training_stats(training_stats, \"training_report\")","block_group":"8a4be897a65d4e7b92ba1b9540a66a1e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1fe5TlpOGgc","colab":{"height":533,"base_uri":"https://localhost:8080/"},"outputId":"bd662849-671c-479d-d8ce-a5aad3ed0f76","cell_id":"8e83a1affc644e54a77635eb49e16610","deepnote_cell_type":"code"},"source":"visualize_policy(reward_function, policy)","block_group":"882475b5720a497280a31771bffba5a7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvTF2EBTB3Ur","cell_id":"9cac2e9b7e8d4e5e9666323084f8257c","deepnote_cell_type":"markdown"},"source":"Now if you have extra time, you may want to modify the reward function to learn more complicated skills. For example, you may learn the box flipping skill from the force control chapter using REINFORCE method!","block_group":"cdedb64b05c64e46acd46ac0ae6fc278"},{"cell_type":"markdown","metadata":{"id":"3u1YKG0LOGgd","cell_id":"4639a2657ecf4aae9c919affcb577e77","deepnote_cell_type":"markdown"},"source":"Weng, L. (2018, April 08). Policy Gradient Algorithms. Retrieved November 19, 2020, from https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html","block_group":"fb86a1269f97452b90db955eede41aa9"},{"cell_type":"markdown","metadata":{"id":"MwE8yNg58VQN","cell_id":"8a6f89f6c9864dba84e1e36c29263afe","deepnote_cell_type":"markdown"},"source":"## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza.\n\nFor submission of this assignment, you must do two things.\n- Download and submit the notebook `policy_gradient.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [2 pts] Correct implementation of `util_compute_policy_loss`\n- [2 pts] Correct implementation of `util_compute_value_loss`\n- [4 pts] Correct implementation of `compute_advantages`","block_group":"eabae7c8e16a4770a8afa60b0218dd06"},{"cell_type":"code","metadata":{"id":"vb_OFdG9Zt7K","cell_id":"9d2d13a28965437ba974f2f7dc0060ab","deepnote_cell_type":"code"},"source":"Grader.grade_output([TestVPG], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"f348f55bb8334343b24aee1fa6cd7c35","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=8ac7c900-70d2-4af1-83c6-341a64fb0e14' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.10.12"},"deepnote_notebook_id":"45cc79fd37c644628bcd2205a550f0d6","deepnote_execution_queue":[]}}