{"cells":[{"cell_type":"markdown","metadata":{"id":"w7C_Q2UbkGas","cell_id":"178738e6fcd34baf810fbd98c63fd6d9","deepnote_cell_type":"markdown"},"source":"## Stochastic Optimization","block_group":"e46b73b334564c9c9a317de35ebfbd96"},{"cell_type":"code","metadata":{"id":"nLnz0sRrSjOg","cell_id":"34e5d645588f4335b189fdc3723cb907","deepnote_cell_type":"code"},"source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport mpld3\nimport numpy as np\nfrom manipulation import running_as_notebook\nfrom pydrake.all import (\n    BaseField,\n    Evaluate,\n    Fields,\n    PointCloud,\n    Rgba,\n    RigidTransform,\n    Sphere,\n    StartMeshcat,\n    Variable,\n)\n\nif running_as_notebook:\n    mpld3.enable_notebook()","block_group":"fcf896912f3f49258c7a08a89389deaa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"fe7f76dd08734a7eaf825b7215e1d730","deepnote_cell_type":"code"},"source":"def loss(theta):\n    x = theta[0]\n    y = theta[1]\n    eval = 2 * x**2 - 1.05 * x**4 + x**6 / 6 + x * y + y**2\n    return 0.25 * eval\n\n\ndef generate_color_mat(color_vec, shape):\n    color_mat = np.tile(\n        np.array(color_vec).astype(np.float32).reshape(3, 1), (1, shape[1])\n    )\n    return color_mat\n\n\ndef visualize_loss(\n    meshcat,\n    loss,\n    colormap=\"viridis\",\n    spacing=0.01,\n    clip_min=None,\n    clip_max=None,\n):\n    # Create a grid of thetas and evaluate losses.\n    points = []\n    for i in np.arange(-3, 3, spacing):\n        for j in np.arange(-3, 3, spacing):\n            points.append([i, j, loss(np.array([i, j]))])\n    points = np.array(points)\n\n    # Normalize losses and color them according to colormap.\n    cmap = matplotlib.cm.get_cmap(colormap)\n    min_loss = np.min(points[:, 2]) if clip_min == None else clip_min\n    max_loss = np.max(points[:, 2]) if clip_max == None else clip_max\n\n    colors = []\n    for i in range(points.shape[0]):\n        normalized_loss = (points[i, 2] - min_loss) / (max_loss - min_loss)\n        colors.append(list(cmap(normalized_loss))[0:3])\n\n    cloud = PointCloud(\n        points.shape[0], Fields(BaseField.kXYZs | BaseField.kRGBs)\n    )\n    cloud.mutable_xyzs()[:] = points.T\n    cloud.mutable_rgbs()[:] = 255 * np.array(colors).T\n\n    meshcat.Delete()\n    meshcat.SetProperty(\"/Background\", \"visible\", False)\n    meshcat.SetObject(\"/loss\", cloud, point_size=0.03)\n\n\ndef visualize_trajectory(trajectory):\n    points = PointCloud(trajectory.shape[0])\n    points.mutable_xyzs()[:] = trajectory.T\n    meshcat.SetObject(\"/traj\", points, rgba=Rgba(1, 0, 0), point_size=0.03)\n    meshcat.SetLine(\"/traj_line\", trajectory.T, rgba=Rgba(1, 0, 0))\n\n    # Visualize the initial guess.\n    meshcat.SetObject(\"/traj_initial\", Sphere(0.05), Rgba(1, 0, 0))\n    meshcat.SetTransform(\"/traj_initial\", RigidTransform(trajectory[0, :]))\n\n    # Visualize the final point of the iteration.\n    meshcat.SetObject(\"/traj_final\", Sphere(0.05), Rgba(0, 1, 0))\n    meshcat.SetTransform(\"/traj_final\", RigidTransform(trajectory[-1, :]))","block_group":"7adab8ed93ed4957875f08136f3e22bd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"31b108b4fc9a4ec4a1133e21eea085ff","deepnote_cell_type":"code"},"source":"# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"816399b6b6634a0e9f1dad21ab3f5a8e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cUTNJkCK1IDH","cell_id":"3407b0ac939d4df982e49cebf267d5bc","deepnote_cell_type":"markdown"},"source":"## The Three Hump Camel \nIn this exercise, we'll implement our own versions of gradient descent and stochastic gradient descent! \n\nOur goal is to find the minima of the following function:\n\n$$l(x)=\\frac{1}{4}\\bigg(2x_1^2-1.05x_1^4+\\frac{x_1^6}{6}+x_1x_2+x_2^2\\bigg)$$\n\nNote: this function is defined above as `loss(x)`.\n\nWe have visualized the landscape of this function in meshcat if you run the cell below! You will notice the following things:\n\n1. This function has 3 local minima (hence, the name 'three hump camel')\n2. The global minima is located at $f([0,0])=0$. ","block_group":"4c222cbf8a4147a183311925f946265d"},{"cell_type":"code","metadata":{"id":"U1GCQpPf1HwO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"04be0d22-a216-4ee5-b7df-261b10f3320b","cell_id":"49947cefb05a4db68bc414e3afa27860","deepnote_cell_type":"code"},"source":"# The parameters are optimized for best visualization in meshcat.\n# For faster visualization, try increasing spacing.\nvisualize_loss(meshcat, loss, colormap=\"viridis\", spacing=0.02, clip_max=2.0)","block_group":"d3c7ab6846704df088dc445760ba14f4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nBpUQcfOcwF","cell_id":"2975a9a3be7940fc825328aa3f48ab12","deepnote_cell_type":"markdown"},"source":"## Gradient Descent\n\nAs we saw in the lecture, one way of trying to find the minimum of $l(x)$ is to use explicit gradients and do gradient descent. \n\n$$x \\leftarrow x - \\eta\\bigg(\\frac{\\partial l(x)}{\\partial x}\\bigg)^T$$\n\nWe've set up a basic outline of the gradient descent algoritm for you. Take a look at the following function `gradient_descent` that implements the following steps:\n\n1. Initialize $x\\in\\mathbb{R}^2$ at random from some bounded region.\n2. Until maximum iteration, update $x$ according to some update rule like the one defined above. \n\nThroughout the following notebook, we will walk-through a handful of potential update functions.","block_group":"262aff5e8f2d4015a03e797b66ac698b"},{"cell_type":"code","metadata":{"id":"pH6DEMMA9cXP","cell_id":"e9bbced88f4f466ea355120096709732","deepnote_cell_type":"code"},"source":"def gradient_descent(rate, update_rule, initial_x=None, iter=1000):\n    \"\"\"gradient descent algorithm\n    @params:\n    - rate (float): eta variable of gradient descent.\n    - update_rule: a function with a signature update_rule(x, rate).\n    - initial_x: initial position for gradient descent.\n    - iter: number of iterations to run gradient descent for.\n    \"\"\"\n    # If no initial guess is supplied, then randomly choose one.\n    if initial_x is None:\n        x = -3 + 6.0 * np.random.rand(2)\n    else:\n        x = initial_x\n    # Compute loss for first parameter for visualization.\n    x_list = []\n    x_list.append([x[0], x[1], loss(x)])\n    # Loop through with gradient descent.\n    for i in range(iter):\n        # Update the parameters using update rule.\n        x = update_rule(x, rate)\n        x_list.append([x[0], x[1], loss(x)])\n    return np.array(x_list)","block_group":"9047472b65e440039a8a25ce4b8b154f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcIyb-iJRGHg","cell_id":"bb186ad9ab22429abe4ec3fa9480c73c","deepnote_cell_type":"markdown"},"source":"## Determinisitc Exact Gradients\n\n**Problem 11.1.a** [2 pts]: Let's first use the standard gradient descent algorithm with exact gradients. Below, you must implement the following simple update function:\n\n$$x \\leftarrow x - \\eta\\bigg(\\frac{\\partial l(x)}{\\partial x}\\bigg)^T$$\n\nHINT: You can write down the gradient yourself, but remember you can also use Drake's symbolic differentiation!\n","block_group":"32743b884f194ef3bb4d199f26fb0535"},{"cell_type":"code","metadata":{"id":"kO7h13kCUc1a","cell_id":"87d2a663849f4b6bafc9fac8e9f34750","deepnote_cell_type":"code"},"source":"def exact_gradient(x, rate):\n    \"\"\"\n    Update rule. Receive theta and update it with the next theta.\n    Input:\n        - x: input variable x.\n        - rate: rate of descent, variable \"eta\".\n    Output:\n        - x: updated variable x.\n    \"\"\"\n\n    # YOUR CODE HERE\n\n    return x","block_group":"4add19dca5614616a65a3e5542583a2f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LT1PCqWPTTy2","cell_id":"2fa0d71861fa4796adb9e9152d6c1ac1","deepnote_cell_type":"markdown"},"source":"When you've completed the function, you can run the below cell to check the visualization! For this problem, the visualization has the following convention:\n- Red sphere is the initial guess \n- Green sphere is the final point after `iter` iterations. \n- Every updated parameter is drawn as smaller red cubes. ","block_group":"229cb9b237be4276b5a05c479a3ed559"},{"cell_type":"code","metadata":{"id":"rEG6dKaxTbie","cell_id":"d0f4b97ca6d14503868f242457f14570","deepnote_cell_type":"code"},"source":"# Compute the trajectory.\ntrajectory = gradient_descent(0.1, exact_gradient)\nvisualize_trajectory(trajectory)","block_group":"168e5ebea57448b4bbf48f8049f076d9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QT4O4yL7iuNg","cell_id":"8d7b44d3be654955be885b9beedd52a0","deepnote_cell_type":"markdown"},"source":"If you've implemented it correctly, run the cell multiple times to see the behavior of gradient descent from different initial conditions. You should note that depending on where you started, you are deterministically stuck in the local minima that corresponds to its attraction region. ","block_group":"cc62d651e189447cbc131f8cfc10d0f2"},{"cell_type":"markdown","metadata":{"id":"2V8LydfhVMdJ","cell_id":"e796d218b1324ca1aa4b3ccee34e64df","deepnote_cell_type":"markdown"},"source":"## Stochastic Approximation to Gradients\n\n**Problem 11.1.b** [2 pts]: One of the mindblowing facts we learned from the lecture was that we can actually do gradient descent without ever having true gradients of the loss function $l(x)$! We will demonstrate this here with a discrete approximation of our loss function's derivative.\n\nYour job is to write down the following update function for gradient descent:\n\n$$x \\leftarrow x - \\eta\\big[l(x+w)-l(x)\\big]w$$\n\nwhere $w\\in\\mathbb{R}^2$ drawn from a Gaussian distribution, $w\\sim\\mathcal{N}(0,\\sigma^2=0.25)$. You can use `np.random.normal()` to draw from this distribution.","block_group":"c1abd5f7c92341a2a65ac14630f37e60"},{"cell_type":"code","metadata":{"id":"leVxvWu3lLYd","cell_id":"be09ad69f7774f55a7ca820875d6dcb0","deepnote_cell_type":"code"},"source":"def approximated_gradient(x, rate):\n    \"\"\"\n    Update rule. Receive theta and update it with the next theta.\n    Input:\n        - x: input variable x.\n        - rate: rate of descent, variable \"eta\".\n    Output:\n        - x: updated variable x.\n    \"\"\"\n\n    # YOUR CODE HERE\n\n    return x","block_group":"bb4ff82b20eb42e1a1ac5d56e558514e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tg3ek5nz1ioL","cell_id":"62db1112f4f146738826dbca6733cff3","deepnote_cell_type":"markdown"},"source":"Again, once you've implemented the function, run the below cell to visualize the trajectory.","block_group":"d9c14ed9d073435fb3ffda34e1c681cb"},{"cell_type":"code","metadata":{"id":"yku6xDTQQtAt","cell_id":"a98f22547e274b18bcff043a48385611","deepnote_cell_type":"code"},"source":"trajectory = gradient_descent(0.01, approximated_gradient, iter=10000)\nvisualize_trajectory(trajectory)","block_group":"ff21ad0861da4b6a85f618a6f3dd4092","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTEL3_ENl1oI","cell_id":"070cf63e629a4ca48a136ee1737bea75","deepnote_cell_type":"markdown"},"source":"If you've implemented the function correctly, take a moment to run it from multiple different conditions - the results should be somewhat shocking.\n- With the right parameters ($\\sigma,\\eta$), this version of gradient descent is much better than the deterministic exact version at converging to global minima. (In fact, you'll sometimes see it hop out of one of the local minimas and converge to a global minima?)\n- But we never explicitly took derivatives!\n- (Side note): does this mean this way approximating gradients is the magical tool to everything? not quite. This version can be prone to getting stuck in saddle points!","block_group":"50dcd2989b374cb89f7f90d1dd47fa08"},{"cell_type":"markdown","metadata":{"id":"NNRWAjIjmiVV","cell_id":"6cae1e38caba4565acfedb9eec0dc99e","deepnote_cell_type":"markdown"},"source":"## Baselines \n\n**Problem 11.1.c** [4 pts]: We don't necessarily have to take finite differences to estimate the gradient. In fact, we could have subtracted our perturbed estimate from any function, as long as that is not a function of $w$! Consider the following update function:\n\n$$x \\leftarrow x - \\eta\\big[l(x+w)-b(x)\\big]w$$\n\nProve that on average, the difference in updates (call it $\\mathbb{E}_w[\\Delta x$]) from this function is approximately equal to the true analytical gradient. Provide your answer in your written submission for this problem set.\n\nHINT: You should use the first-order taylor approximation of $l(x+w)$ (and you may assume $w$ is quite small)","block_group":"804e7a05de294aadb6941f360d7ef35e"},{"cell_type":"markdown","metadata":{"id":"zdJXocpw3OTb","cell_id":"30c69fa98f9747b28a082635cdb185cf","deepnote_cell_type":"markdown"},"source":"**Problem 11.1.d** [1 pts]: Finally, implement the update law from above. The update rule is almost identical to 11.1.b except for the implementation of the baseline, so this is like a bonus question.  ","block_group":"b65dcc50e26c4ccfaf09a946c95353af"},{"cell_type":"code","metadata":{"id":"4vjty4Tc9bZw","cell_id":"71e110dbf1384315a658d1cd397883c6","deepnote_cell_type":"code"},"source":"def approximated_gradient_with_baseline(x, rate, baseline):\n    \"\"\"\n    Update rule. Receive theta and update it with the next theta.\n    Input:\n        - x: input variable x.\n        - rate: rate of descent, variable \"eta\".\n        - baseline: float for baseline.\n    Output:\n        - x: updated variable x.\n    \"\"\"\n\n    # YOUR CODE HERE\n\n    return x","block_group":"e1fc9fc38eef48c4816407162fa90d2a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9N1BgRG29jNV","cell_id":"8d8a4ce492954a718df03a15c41a2e37","deepnote_cell_type":"markdown"},"source":"As you proved in 11.1.c, adding a baseline does not change the mean of the update. However, it does change the variance!\n\nIn the below code, you can play around with different values of the baseline to see what happens. Remember that the optimal value (smallest variance) of the baseline is $l(x)$. \n\nYou should see that if the baseline is close to `loss(x)` (e.g. baseline is uniformly zero), there is no big difference with the solution you wrote on 11.1.b. However, when the baseline is far from `loss(x)` (e.g. baseline is uniformly 5), our path starts to look more like a random walk due to high variance.","block_group":"df2a4782d98c469291231e567bcf5b71"},{"cell_type":"code","metadata":{"id":"0IKbIy1P6a4k","cell_id":"c33bed24bb8e45dfaa1d8e639d941638","deepnote_cell_type":"code"},"source":"def baseline(x):\n    return 5  # feel free to modify here!\n\n\ndef reduced_function(x, rate):\n    return approximated_gradient_with_baseline(x, rate, baseline)\n\n\ntrajectory = gradient_descent(0.01, reduced_function, iter=10000)\nvisualize_trajectory(trajectory)","block_group":"495c5ffacf574e76bfdd95230cbfc659","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MwE8yNg58VQN","cell_id":"5bc158050a934070accb7e01e1825d8c","deepnote_cell_type":"markdown"},"source":"## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza. \n\nFor submission of this assignment, you must do two things. \n- Download and submit the notebook `stochastic_optimization.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n- Write down your answers to 11.1.c in your PDF submission to Gradescope. \n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [2 pts] 11.1.a must be implemented correctly.\n- [2 pts] 11.1.b must be implemented correctly.\n- [4 pts] 11.1.c is answered correctly.\n- [1 pts] 11.1.d must be implemented correctly.","block_group":"ba7a2383d3ca4a5a8c365aec24d33fc4"},{"cell_type":"code","metadata":{"id":"pQISVdEG9NoN","colab":{"height":389,"base_uri":"https://localhost:8080/"},"outputId":"8ecd274d-3fd8-4d2f-9fe3-fc530e8207b0","cell_id":"d797944898004244a196671382fdc8dc","deepnote_cell_type":"code"},"source":"from manipulation.exercises.rl.test_stochastic_optimization import (\n    TestStochasticOptimization,\n)\nfrom manipulation.exercises.grader import Grader\n\nGrader.grade_output([TestStochasticOptimization], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"bff81775e5aa4863bb8e2fe6b865a1cd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=8ac7c900-70d2-4af1-83c6-341a64fb0e14' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stochastic_optimization.ipynb","provenance":[],"collapsed_sections":[]},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"celltoolbar":"Tags","language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"56dff700ed9c49509ddfe460ee5be170","deepnote_execution_queue":[]}}